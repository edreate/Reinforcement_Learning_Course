{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic\n",
    "\n",
    "#### Off Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions: 4\n",
      "Actions:\n",
      "0: do nothing\n",
      "1: fire left orientation engine\n",
      "2: fire main engine\n",
      "3: fire right orientation engine\n",
      "\n",
      "Number of state observations: 8\n",
      "State (Observation Space):\n",
      "x, y\n",
      "vel_x, vel_y\n",
      "angle, angle_vel\n",
      "left_leg_touching, right_leg_touching\n",
      "      \n",
      "Current state:  [-1.4054298e-03  1.4162291e+00 -1.4237092e-01  2.3594956e-01\n",
      "  1.6353275e-03  3.2249160e-02  0.0000000e+00  0.0000000e+00]\n",
      "Units of the state are as follows:\n",
      "      ‘x’: (units), ‘y’: (units), \n",
      "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
      "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# Select Lunar Lander v3 as environment\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = int(env.action_space.n)\n",
    "print(f\"Number of possible actions: {n_actions}\")\n",
    "print(\"\"\"Actions:\n",
    "0: do nothing\n",
    "1: fire left orientation engine\n",
    "2: fire main engine\n",
    "3: fire right orientation engine\n",
    "\"\"\")\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(f\"Number of state observations: {n_observations}\")\n",
    "\n",
    "print(\"\"\"State (Observation Space):\n",
    "x, y\n",
    "vel_x, vel_y\n",
    "angle, angle_vel\n",
    "left_leg_touching, right_leg_touching\n",
    "      \"\"\")\n",
    "print(\"Current state: \", state)\n",
    "\n",
    "print(\"\"\"Units of the state are as follows:\n",
    "      ‘x’: (units), ‘y’: (units), \n",
    "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
    "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment to get the initial state\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(50):\n",
    "    env.step(action=0)\n",
    "# Render the environment to get an RGB image\n",
    "frame = env.render()\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Lunar Lander Environment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def plot_metrics(episode_durations, rewards, show_result=False, save_path=None):\n",
    "    # Create a horizontal figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5), dpi=100)\n",
    "    fig.suptitle(\"Training Metrics\" if not show_result else \"Results\", fontsize=16)\n",
    "\n",
    "    # Plot Episode Durations\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    axes[0].set_title(\"Episode Durations\")\n",
    "    axes[0].set_xlabel(\"Episode\")\n",
    "    axes[0].set_ylabel(\"Duration\")\n",
    "    axes[0].plot(durations_t.numpy(), label=\"Duration\")\n",
    "\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        axes[0].plot(means.numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot Rewards\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float)\n",
    "    axes[1].set_title(\"Rewards\")\n",
    "    axes[1].set_xlabel(\"Episode\")\n",
    "    axes[1].set_ylabel(\"Reward\")\n",
    "    axes[1].plot(rewards_t.numpy(), label=\"Reward\")\n",
    "\n",
    "    if len(rewards_t) >= 100:\n",
    "        reward_means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        reward_means = torch.cat((torch.zeros(99), reward_means))\n",
    "        axes[1].plot(reward_means.numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Adjust layout and save/show\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add space for the suptitle\n",
    "    if save_path:\n",
    "        plt.savefig(save_path + \".png\", dpi=300)\n",
    "        print(f\"Metrics figure saved to {save_path}\")\n",
    "\n",
    "    if \"get_ipython\" in globals():\n",
    "        if not show_result:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(fig)\n",
    "        else:\n",
    "            display.display(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from typing import List, Deque\n",
    "\n",
    "# Define the type of the Transition tuple\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "\n",
    "# ReplayMemory class with strong type hints\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int):\n",
    "        # The deque stores Transition objects\n",
    "        self.memory: Deque[Transition] = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, state: float, action: int, next_state: float, reward: float, done: bool) -> None:\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(state, action, next_state, reward, int(done)))\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[Transition]:\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the current size of the memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from models import MLP\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cc742b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Weight Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target: nn.Module, source: nn.Module, tau: float) -> None:\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "\n",
    "def hard_update(target: nn.Module, source: nn.Module) -> None:\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for policy-based reinforcement learning.\n",
    "\n",
    "    Architecture:\n",
    "    - Input layer: Accepts `num_inputs` features representing the state.\n",
    "    - Hidden layers: Two fully connected layers with 256 units each and ReLU activation for non-linearity.\n",
    "    - Output layer: Produces `num_outputs`, representing action space size or logits.\n",
    "\n",
    "    Args:\n",
    "        num_inputs (int): Number of input features (state size).\n",
    "        num_outputs (int): Number of output features (action size).\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Propagates the input through the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs: int, num_outputs: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 256), nn.ReLU(), nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, num_inputs).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_outputs).\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_observations: int, num_actions: int):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.q1 = MLP(input_dim=num_observations + num_actions, output_dim=1)\n",
    "        self.q2 = MLP(input_dim=num_observations + num_actions, output_dim=1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor):\n",
    "        xu = torch.cat([state, action], 1)\n",
    "\n",
    "        x1 = self.q1(xu)\n",
    "        x2 = self.q2(xu)\n",
    "\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "policy_network = PolicyNetwork(n_observations, n_actions)\n",
    "\n",
    "# Q Network Q1 and Q2\n",
    "q_network = QNetwork(n_observations, n_actions)\n",
    "\n",
    "# Target Q Network Q1_target and Q2_target\n",
    "q_network_target = QNetwork(n_observations, n_actions)\n",
    "\n",
    "# Hard update target initially so that Q networks have same weights in the beginning\n",
    "hard_update(q_network_target, q_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftActorCriticAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy_network: nn.Module,\n",
    "        critic_network: nn.Module,\n",
    "        target_critic_network: nn.Module,\n",
    "        discount_factor_gamma: float = 0.99,\n",
    "        soft_update_rate_tau: float = 0.05,\n",
    "        exploration_temperature_alpha: float = 0.2,\n",
    "        normalize_rewards: bool = True,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        learning_rate: float = 3e-3,\n",
    "    ) -> None:\n",
    "        self.discount_factor_gamma = discount_factor_gamma\n",
    "        self.soft_update_rate_tau = soft_update_rate_tau\n",
    "        self.exploration_temperature_alpha = exploration_temperature_alpha\n",
    "\n",
    "        self.policy_network = policy_network\n",
    "        self.critic_network = critic_network\n",
    "        self.target_critic_network = target_critic_network\n",
    "\n",
    "    def sample_stochastic_action_with_log_prob(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Stochastically selects an action and computes its log-probability for discrete SAC.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, torch.Tensor]:\n",
    "                - action (int): The chosen action index.\n",
    "                - log_prob (torch.Tensor): The log-probability of the chosen action.\n",
    "        \"\"\"\n",
    "        action_logits = self.policy_network(state)  # Get logits from the policy network\n",
    "        action_distribution = Categorical(logits=action_logits)  # Create a categorical distribution\n",
    "        sampled_action = action_distribution.sample()  # Sample an action\n",
    "        log_prob = action_distribution.log_prob(sampled_action)  # Compute log-probability\n",
    "        return int(sampled_action.item()), log_prob\n",
    "\n",
    "    def sample_greedy_action(self, state: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Selects the most likely action deterministically for evaluation.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state.\n",
    "\n",
    "        Returns:\n",
    "            int: The action index with the highest probability.\n",
    "        \"\"\"\n",
    "        action_logits = self.policy_network(state)  # Get logits from the policy network\n",
    "        greedy_action = torch.argmax(action_logits).item()  # Select the action with the highest logit\n",
    "        return int(greedy_action)\n",
    "\n",
    "    def calculate_policy_loss(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the policy loss for the actor in Soft Actor-Critic (SAC) for discrete action spaces.\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): Batch of states for which to compute the loss.\n",
    "            alpha (float): Entropy temperature coefficient.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The mean policy loss for the actor.\n",
    "        \"\"\"\n",
    "        # Get action logits and log probabilities for all actions\n",
    "        action_logits = self.policy_network(states)\n",
    "        action_distribution = Categorical(logits=action_logits)\n",
    "        log_pis = action_distribution.logits  # Log-probabilities for all actions\n",
    "        action_probs = action_distribution.probs  # Action probabilities\n",
    "\n",
    "        # Evaluate Q-values for all actions from both critics\n",
    "        q1_values, q2_values = self.critic_network(states)\n",
    "\n",
    "        # Take the minimum of the two Q-values for stability\n",
    "        min_q_values = torch.min(q1_values, q2_values)\n",
    "\n",
    "        # Compute the weighted loss\n",
    "        # (α * log_pis - Q(s, a)) weighted by the action probabilities\n",
    "        weighted_loss = (action_probs * (self.exploration_temperature_alpha * log_pis - min_q_values)).sum(dim=1)\n",
    "        policy_loss = weighted_loss.mean()\n",
    "\n",
    "        return policy_loss\n",
    "\n",
    "    def calculate_q_value_loss(\n",
    "        self,\n",
    "        states: torch.Tensor,\n",
    "        actions: torch.Tensor,\n",
    "        rewards: torch.Tensor,\n",
    "        next_states: torch.Tensor,\n",
    "        dones: torch.Tensor,\n",
    "        gamma: float,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the Q-value loss for the critics in Soft Actor-Critic (SAC).\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): Current states (shape: [batch_size, state_dim]).\n",
    "            actions (torch.Tensor): Actions taken in the current states (shape: [batch_size]).\n",
    "            rewards (torch.Tensor): Rewards received after taking the actions (shape: [batch_size]).\n",
    "            next_states (torch.Tensor): Next states resulting from the actions (shape: [batch_size, state_dim]).\n",
    "            dones (torch.Tensor): Terminal flags for the episodes (shape: [batch_size]).\n",
    "            gamma (float): Discount factor for future rewards.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The combined Q-value loss for both Q-networks.\n",
    "        \"\"\"\n",
    "        # Compute current Q-values for the selected actions\n",
    "        q1_values, q2_values = self.critic_network(states)  # Both Q-network outputs (shape: [batch_size, num_actions])\n",
    "        q1_current = q1_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Q1 for taken actions\n",
    "        q2_current = q2_values.gather(1, actions.unsqueeze(1)).squeeze(1)  # Q2 for taken actions\n",
    "\n",
    "        # Compute target Q-values using the Bellman equation\n",
    "        with torch.no_grad():\n",
    "            # Compute target policy's probabilities and log probabilities\n",
    "            next_action_logits = self.policy_network(next_states)\n",
    "            next_action_distribution = Categorical(logits=next_action_logits)\n",
    "            next_action_probs = next_action_distribution.probs\n",
    "            next_action_log_probs = next_action_distribution.logits\n",
    "\n",
    "            # Compute the target Q-values for next states using the minimum of the critics\n",
    "            next_q1_values, next_q2_values = self.critic_network(next_states)\n",
    "            next_q_values = torch.min(next_q1_values, next_q2_values)  # Min Q-values for next state\n",
    "\n",
    "            # Compute the expected Q-value for the next state\n",
    "            target_q_values = torch.sum(\n",
    "                next_action_probs * (next_q_values - self.exploration_temperature_alpha * next_action_log_probs), dim=1\n",
    "            )\n",
    "            q_value_target = rewards + gamma * (1 - dones) * target_q_values\n",
    "\n",
    "        # Compute the Mean Squared Error (MSE) loss for both critics\n",
    "        qf1_loss = F.mse_loss(q1_current, q_value_target)\n",
    "        qf2_loss = F.mse_loss(q2_current, q_value_target)\n",
    "\n",
    "        # Combine the losses\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        return qf_loss\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        replay_buffer: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "        batch_size: int,\n",
    "        policy_optimizer: torch.optim.Optimizer,\n",
    "        critic_optimizer: torch.optim.Optimizer,\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Performs an optimization step for the policy and critic networks using data from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            replay_buffer (Tuple): A tuple containing states, actions, rewards, next_states, and dones.\n",
    "            batch_size (int): The batch size for sampling.\n",
    "            policy_optimizer (torch.optim.Optimizer): Optimizer for the policy network.\n",
    "            critic_optimizer (torch.optim.Optimizer): Optimizer for the critic network.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Policy loss and Q-value loss as Python floats.\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = replay_buffer\n",
    "\n",
    "        # Sample a batch from the replay buffer\n",
    "        idx = np.random.choice(len(states), batch_size, replace=False)\n",
    "        states_batch = states[idx].to(self.device)\n",
    "        actions_batch = actions[idx].to(self.device)\n",
    "        rewards_batch = rewards[idx].to(self.device)\n",
    "        next_states_batch = next_states[idx].to(self.device)\n",
    "        dones_batch = dones[idx].to(self.device)\n",
    "\n",
    "        # Calculate Q-value loss\n",
    "        q_value_loss = self.calculate_q_value_loss(\n",
    "            states=states_batch,\n",
    "            actions=actions_batch,\n",
    "            rewards=rewards_batch,\n",
    "            next_states=next_states_batch,\n",
    "            dones=dones_batch,\n",
    "            gamma=self.discount_factor_gamma,\n",
    "        )\n",
    "\n",
    "        # Update critic\n",
    "        critic_optimizer.zero_grad()\n",
    "        q_value_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # Calculate policy loss\n",
    "        policy_loss = self.calculate_policy_loss(states_batch)\n",
    "\n",
    "        # Update policy\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # Perform soft update for target network\n",
    "        with torch.no_grad():\n",
    "            for target_param, param in zip(self.target_critic_network.parameters(), self.critic_network.parameters()):\n",
    "                target_param.data.copy_(\n",
    "                    self.soft_update_rate_tau * param.data + (1.0 - self.soft_update_rate_tau) * target_param.data\n",
    "                )\n",
    "\n",
    "        return policy_loss.item(), q_value_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_network = PolicyNetwork(n_observations, n_actions)\n",
    "\n",
    "critic_network = MLP(input_dim=n_observations, output_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 9e-3\n",
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic_agent = ActorCriticAgent(\n",
    "    policy_network=actor_network,\n",
    "    critic_network=critic_network,\n",
    "    discount_factor=discount_factor,\n",
    "    device=device,\n",
    "    learning_rate=learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_on_batch(env: gym.Env, agent: ActorCriticAgent, device: torch.device, batch_size: int = 16) -> dict:\n",
    "    \"\"\"\n",
    "    Collects data from multiple episodes and trains the policy on the combined batch.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to interact with (following the OpenAI Gym interface).\n",
    "        policy: The policy object that defines action selection and optimization.\n",
    "        device: The device to run computations on (CPU/GPU).\n",
    "        batch_size: Number of episodes to collect before updating the policy.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of metrics tracking batch performance and training progress.\n",
    "    \"\"\"\n",
    "    global global_track_steps_per_episode, global_track_reward_per_episode\n",
    "\n",
    "    # Lists to store batch data\n",
    "    batch_log_probs = []\n",
    "    batch_rewards = []\n",
    "    batch_returns = []\n",
    "    batch_values = []\n",
    "\n",
    "    total_rewards = []\n",
    "    total_steps = 0\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        # Reset the environment for a new episode\n",
    "        state, _ = env.reset()\n",
    "        episode_log_probs = []\n",
    "        episode_rewards = []\n",
    "        episode_values = []\n",
    "\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "\n",
    "        while True:\n",
    "            # Convert state to tensor and send it to the device\n",
    "            state_tensor = torch.Tensor(state).to(device)\n",
    "\n",
    "            # Select an action using the policy\n",
    "            action, log_prob = agent.select_action(state=state_tensor)\n",
    "\n",
    "            _, value_pred = agent.actor_critic(state_tensor)\n",
    "\n",
    "            # Take the selected action in the environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            episode_steps += 1\n",
    "\n",
    "            # Store log-probability and reward\n",
    "            episode_log_probs.append(log_prob)\n",
    "            episode_rewards.append(reward)\n",
    "            episode_values.append(value_pred)\n",
    "\n",
    "            # Update cumulative reward and state\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "\n",
    "            # Break if the episode ends\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        global_track_steps_per_episode.append(episode_steps)\n",
    "        global_track_reward_per_episode.append(episode_reward)\n",
    "\n",
    "        plot_metrics(global_track_steps_per_episode, global_track_reward_per_episode)\n",
    "\n",
    "        # Store episode data into batch data\n",
    "        total_rewards.append(episode_reward)\n",
    "        batch_log_probs.extend(episode_log_probs)\n",
    "        batch_rewards.extend(episode_rewards)\n",
    "        batch_values.extend(episode_values)\n",
    "\n",
    "        # Compute discounted returns for the episode and store them\n",
    "        episode_returns = agent.compute_discounted_returns(torch.Tensor(episode_rewards))\n",
    "        batch_returns.extend(episode_returns)\n",
    "\n",
    "    # Convert batch data to tensors\n",
    "    batch_log_probs_tensor = torch.stack(batch_log_probs).to(device)\n",
    "    batch_returns_tensor = torch.Tensor(batch_returns).to(device)\n",
    "    batch_values_tensor = torch.stack(batch_values).to(device)\n",
    "\n",
    "    # Optimize the policy\n",
    "    policy_loss, critic_loss = agent.optimize(batch_log_probs_tensor, batch_returns_tensor, batch_values_tensor)\n",
    "\n",
    "    # Return metrics to track training progress\n",
    "    metrics = {\n",
    "        \"batch_reward\": sum(total_rewards) / batch_size,  # Average reward per episode\n",
    "        \"total_steps\": total_steps // batch_size,\n",
    "        \"policy_loss\": policy_loss,\n",
    "        \"critic_loss\": critic_loss,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vpg_policy(\n",
    "    max_episodes: int = 500, reward_threshold: float = 200.0, rolling_window: int = 50, batch_size: int = 16\n",
    "):\n",
    "    rewards_history = []\n",
    "    loss_history = []\n",
    "    steps_history = []\n",
    "\n",
    "    best_average_reward = 0\n",
    "\n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        metrics = train_policy_on_batch(env, actor_critic_agent, device, batch_size)\n",
    "\n",
    "        # Collect metrics\n",
    "        rewards_history.append(metrics[\"batch_reward\"])\n",
    "        loss_history.append(metrics[\"policy_loss\"])\n",
    "        steps_history.append(metrics[\"total_steps\"])\n",
    "\n",
    "        # Print metrics every 50 episodes\n",
    "        avg_reward = np.mean(rewards_history[-50:])\n",
    "\n",
    "        if episode % 5 == 0:\n",
    "            print(\n",
    "                f\"Episode {episode}: Average Reward: {avg_reward:.2f}, \"\n",
    "                f\"Loss: {metrics['policy_loss']:.4f}, Steps: {metrics['total_steps']}\"\n",
    "            )\n",
    "\n",
    "        # Convergence condition: Check if the rolling average exceeds the reward threshold\n",
    "        if len(rewards_history) >= rolling_window:\n",
    "            avg_rolling_reward = np.mean(rewards_history[-rolling_window:])\n",
    "            if avg_rolling_reward >= reward_threshold:\n",
    "                print(\n",
    "                    f\"Environment solved in {episode} episodes! \"\n",
    "                    f\"Average reward over the last {rolling_window} episodes: {avg_rolling_reward:.2f}\"\n",
    "                )\n",
    "                break\n",
    "        if avg_reward >= best_average_reward:\n",
    "            best_average_reward = avg_reward\n",
    "\n",
    "            # Create a filename that includes both the steps_done and timestamp\n",
    "            filename = f\"output/actor_critic_policy_network_lunar_lander_v3_bs_{batch_size}_{timestamp}.pth\"\n",
    "\n",
    "            # Save the policy network with the dynamically generated filename\n",
    "            torch.save(policy.policy_network.state_dict(), filename)\n",
    "\n",
    "            print(f\"Episode: {episode}, Average reward: {avg_reward}. Model saved as: {filename}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return rewards_history, loss_history, steps_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 100\n",
    "reward_threshold = 200.0\n",
    "batch_size = 16\n",
    "\n",
    "global_track_steps_per_episode = []\n",
    "global_track_reward_per_episode = []\n",
    "\n",
    "rewards_history, loss_history, steps_history = train_vpg_policy(\n",
    "    max_episodes=max_episodes, reward_threshold=reward_threshold, batch_size=batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
