{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic\n",
    "\n",
    "#### Off Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions: 4\n",
      "Actions:\n",
      "0: do nothing\n",
      "1: fire left orientation engine\n",
      "2: fire main engine\n",
      "3: fire right orientation engine\n",
      "\n",
      "Number of state observations: 8\n",
      "State (Observation Space):\n",
      "x, y\n",
      "vel_x, vel_y\n",
      "angle, angle_vel\n",
      "left_leg_touching, right_leg_touching\n",
      "      \n",
      "Current state:  [-0.00285234  1.4085072  -0.28892618 -0.10724296  0.00331192  0.065446\n",
      "  0.          0.        ]\n",
      "Units of the state are as follows:\n",
      "      ‘x’: (units), ‘y’: (units), \n",
      "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
      "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# Select Lunar Lander v3 as environment\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = int(env.action_space.n)\n",
    "print(f\"Number of possible actions: {n_actions}\")\n",
    "print(\"\"\"Actions:\n",
    "0: do nothing\n",
    "1: fire left orientation engine\n",
    "2: fire main engine\n",
    "3: fire right orientation engine\n",
    "\"\"\")\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(f\"Number of state observations: {n_observations}\")\n",
    "\n",
    "print(\"\"\"State (Observation Space):\n",
    "x, y\n",
    "vel_x, vel_y\n",
    "angle, angle_vel\n",
    "left_leg_touching, right_leg_touching\n",
    "      \"\"\")\n",
    "print(\"Current state: \", state)\n",
    "\n",
    "print(\"\"\"Units of the state are as follows:\n",
    "      ‘x’: (units), ‘y’: (units), \n",
    "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
    "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment to get the initial state\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(50):\n",
    "    env.step(action=0)\n",
    "# Render the environment to get an RGB image\n",
    "frame = env.render()\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Lunar Lander Environment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def plot_metrics(episode_durations, rewards, show_result=False, save_path=None):\n",
    "    # Create a horizontal figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5), dpi=100)\n",
    "    fig.suptitle(\"Training Metrics\" if not show_result else \"Results\", fontsize=16)\n",
    "\n",
    "    # Plot Episode Durations\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    axes[0].set_title(\"Episode Durations\")\n",
    "    axes[0].set_xlabel(\"Episode\")\n",
    "    axes[0].set_ylabel(\"Duration\")\n",
    "    axes[0].plot(durations_t.numpy(), label=\"Duration\")\n",
    "\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        axes[0].plot(means.numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot Rewards\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float)\n",
    "    axes[1].set_title(\"Rewards\")\n",
    "    axes[1].set_xlabel(\"Episode\")\n",
    "    axes[1].set_ylabel(\"Reward\")\n",
    "    axes[1].plot(rewards_t.numpy(), label=\"Reward\")\n",
    "\n",
    "    if len(rewards_t) >= 100:\n",
    "        reward_means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        reward_means = torch.cat((torch.zeros(99), reward_means))\n",
    "        axes[1].plot(reward_means.numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Adjust layout and save/show\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add space for the suptitle\n",
    "    if save_path:\n",
    "        plt.savefig(save_path + \".png\", dpi=300)\n",
    "        print(f\"Metrics figure saved to {save_path}\")\n",
    "\n",
    "    if \"get_ipython\" in globals():\n",
    "        if not show_result:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(fig)\n",
    "        else:\n",
    "            display.display(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from typing import List, Deque\n",
    "\n",
    "# Define the type of the Transition tuple\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "\n",
    "# ReplayMemory class with strong type hints\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int):\n",
    "        # The deque stores Transition objects\n",
    "        self.memory: Deque[Transition] = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, state: float, action: int, next_state: float, reward: float, done: bool) -> None:\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(state, action, next_state, reward, int(done)))\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[Transition]:\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the current size of the memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from models import MLP\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x31ae88cb0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Weight Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target: nn.Module, source: nn.Module, tau: float) -> None:\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "\n",
    "def hard_update(target: nn.Module, source: nn.Module) -> None:\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for policy-based reinforcement learning.\n",
    "\n",
    "    Architecture:\n",
    "    - Input layer: Accepts `num_inputs` features representing the state.\n",
    "    - Hidden layers: Two fully connected layers with 256 units each and ReLU activation for non-linearity.\n",
    "    - Output layer: Produces `num_outputs`, representing action space size or logits.\n",
    "\n",
    "    Args:\n",
    "        num_inputs (int): Number of input features (state size).\n",
    "        num_outputs (int): Number of output features (action size).\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Propagates the input through the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs: int, num_outputs: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 256), nn.ReLU(), nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, num_inputs).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_outputs).\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_observations: int, num_actions: int):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.q1 = MLP(input_dim=num_observations + num_actions, output_dim=1)\n",
    "        self.q2 = MLP(input_dim=num_observations + num_actions, output_dim=1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor):\n",
    "        xu = torch.cat([state, action], 1)\n",
    "\n",
    "        x1 = self.q1(xu)\n",
    "        x2 = self.q2(xu)\n",
    "\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftActorCriticAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        actor_network: nn.Module,\n",
    "        critic_network: nn.Module,\n",
    "        target_critic_network: nn.Module,\n",
    "        discount_factor_gamma: float = 0.99,\n",
    "        soft_update_rate_tau: float = 0.05,\n",
    "        exploration_temperature_alpha: float = 0.2,\n",
    "        learning_rate: float = 3e-3,\n",
    "    ) -> None:\n",
    "        self.discount_factor_gamma = discount_factor_gamma\n",
    "        self.soft_update_rate_tau = soft_update_rate_tau\n",
    "        self.exploration_temperature_alpha = exploration_temperature_alpha\n",
    "\n",
    "        self.actor_network = actor_network\n",
    "        self.critic_network = critic_network\n",
    "        self.target_critic_network = target_critic_network\n",
    "\n",
    "        self.policy_optimizer = Adam(self.actor_network.parameters(), lr=learning_rate)\n",
    "        self.critic_optimizer = Adam(self.critic_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def sample_stochastic_action_with_log_prob(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Stochastically selects an action and computes its log-probability for discrete SAC.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, torch.Tensor]:\n",
    "                - action (int): The chosen action index.\n",
    "                - log_prob (torch.Tensor): The log-probability of the chosen action.\n",
    "        \"\"\"\n",
    "        action_logits = self.actor_network(state)  # Get logits from the policy network\n",
    "        action_distribution = Categorical(logits=action_logits)  # Create a categorical distribution\n",
    "        sampled_action = action_distribution.sample()  # Sample an action\n",
    "        log_prob = action_distribution.log_prob(sampled_action)  # Compute log-probability\n",
    "        return int(sampled_action.item()), log_prob\n",
    "\n",
    "    def sample_greedy_action(self, state: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Selects the most likely action deterministically for evaluation.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state.\n",
    "\n",
    "        Returns:\n",
    "            int: The action index with the highest probability.\n",
    "        \"\"\"\n",
    "        action_logits = self.actor_network(state)  # Get logits from the policy network\n",
    "        greedy_action = torch.argmax(action_logits).item()  # Select the action with the highest logit\n",
    "        return int(greedy_action)\n",
    "\n",
    "    def estimate_policy_values(self, states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Estimates policy values: action probabilities, log probabilities, and minimum Q-values.\n",
    "\n",
    "        Categorical.probs (Tensor): event probabilities\n",
    "\n",
    "        Categorical.logits (Tensor): event log probabilities (unnormalized)\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): Batch of states for estimation.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - Action probabilities\n",
    "                - Log probabilities of actions\n",
    "                - Minimum Q-values from the critics\n",
    "        \"\"\"\n",
    "        # Get action logits and distribution\n",
    "        action_logits = self.actor_network(states)\n",
    "        action_distribution = Categorical(logits=action_logits)\n",
    "        action_probs = action_distribution.probs  # Action probabilities\n",
    "        log_pis = action_distribution.logits  # Log-probabilities\n",
    "\n",
    "        # Evaluate Q-values for all actions\n",
    "        q1_values, q2_values = self.critic_network(states)\n",
    "        min_q_values = torch.min(q1_values, q2_values)  # Minimum Q-values\n",
    "\n",
    "        return action_probs, log_pis, min_q_values\n",
    "\n",
    "    def calculate_policy_loss(\n",
    "        self, action_probs: torch.Tensor, log_pis: torch.Tensor, min_q_values: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the policy loss.\n",
    "\n",
    "        Args:\n",
    "            action_probs (torch.Tensor): Action probabilities.\n",
    "            log_pis (torch.Tensor): Log probabilities of actions.\n",
    "            min_q_values (torch.Tensor): Minimum Q-values from critics.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Mean policy loss.\n",
    "        #### Mathematical Equation:\n",
    "        For a state $$s$$, the policy loss is:$$\n",
    "        \\mathcal{L}_{\\pi} = \\mathbb{E}_{s \\sim D}\\left[ \\sum_{a} \\pi(a|s) \\left( \\alpha \\log \\pi(a|s) - Q(s, a) \\right) \\right]\n",
    "        $$\n",
    "        \"\"\"\n",
    "        # Weighted loss computation\n",
    "        weighted_loss = (action_probs * (self.exploration_temperature_alpha * log_pis - min_q_values)).sum(dim=1)\n",
    "        policy_loss = weighted_loss.mean()\n",
    "\n",
    "        return policy_loss\n",
    "\n",
    "    def estimate_q_values(\n",
    "        self, states: torch.Tensor, actions: torch.Tensor, next_states: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Estimates current and target Q-values.\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): Current states.\n",
    "            actions (torch.Tensor): Actions taken in the current states.\n",
    "            next_states (torch.Tensor): Next states.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Current Q-values and target Q-values.\n",
    "        \"\"\"\n",
    "        # Current Q-values\n",
    "        q1_values, q2_values = self.critic_network(states)\n",
    "        q1_current = q1_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        q2_current = q2_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_action_logits = self.actor_network(next_states)\n",
    "            next_action_distribution = Categorical(logits=next_action_logits)\n",
    "            next_action_probs = next_action_distribution.probs  # type:ignore\n",
    "            next_action_log_probs = next_action_distribution.logits  # type:ignore\n",
    "\n",
    "            next_q1_values, next_q2_values = self.critic_network(next_states)\n",
    "            next_q_values = torch.min(next_q1_values, next_q2_values)  # type:ignore\n",
    "\n",
    "            target_q_values = torch.sum(\n",
    "                next_action_probs * (next_q_values - self.exploration_temperature_alpha * next_action_log_probs), dim=1\n",
    "            )\n",
    "        return q1_current, q2_current, target_q_values\n",
    "\n",
    "    def calculate_q_value_loss(\n",
    "        self,\n",
    "        q1_current: torch.Tensor,\n",
    "        q2_current: torch.Tensor,\n",
    "        target_q_values: torch.Tensor,\n",
    "        rewards: torch.Tensor,\n",
    "        done: torch.Tensor,\n",
    "        gamma: float,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the Q-value loss.\n",
    "\n",
    "        Args:\n",
    "            q1_current (torch.Tensor): Q1 values for current actions.\n",
    "            q2_current (torch.Tensor): Q2 values for current actions.\n",
    "            target_q_values (torch.Tensor): Target Q-values.\n",
    "            rewards (torch.Tensor): Rewards received after taking actions.\n",
    "            done (torch.Tensor): Terminal flags for the episodes.\n",
    "            gamma (float): Discount factor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Combined Q-value loss for both critics.\n",
    "        #### Mathematical Equation:\n",
    "        For a state $$s$$, action $$a$$, reward $$r$$, next state $$s'$$, and discount factor $$\\gamma$$, the Q-value loss is:$$\n",
    "        \\mathcal{L}_Q = \\mathbb{E}_{(s, a, r, s') \\sim D}\\left[ \\left(Q_{\\theta_1}(s, a) - y\\right)^2 + \\left(Q_{\\theta_2}(s, a) - y\\right)^2 \\right]\n",
    "        $$\n",
    "\n",
    "        where:\n",
    "        $$\n",
    "        y = r + \\gamma (1 - \\text{done}) \\mathbb{E}_{a' \\sim \\pi}\\left[ Q(s', a') - \\alpha \\log \\pi(a'|s') \\right]\n",
    "        $$\n",
    "        \"\"\"\n",
    "        # Target Q-value for Bellman update\n",
    "        q_value_target = rewards + gamma * (1 - done) * target_q_values\n",
    "\n",
    "        # MSE loss\n",
    "        qf1_loss = F.mse_loss(q1_current, q_value_target)\n",
    "        qf2_loss = F.mse_loss(q2_current, q_value_target)\n",
    "\n",
    "        # Combined loss\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "        return qf_loss\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        states_batch: torch.Tensor,\n",
    "        actions_batch: torch.Tensor,\n",
    "        rewards_batch: torch.Tensor,\n",
    "        next_states_batch: torch.Tensor,\n",
    "        done_batch: torch.Tensor,\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Performs an optimization step for the policy and critic networks using data from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            states, actions, rewards, next_states, and done.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Policy loss and Q-value loss as Python floats.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Q-Value Loss Estimation and Optimization\n",
    "        q1_current, q2_current, target_q_values = self.estimate_q_values(\n",
    "            states=states_batch,\n",
    "            actions=actions_batch,\n",
    "            next_states=next_states_batch,\n",
    "        )\n",
    "\n",
    "        q_value_loss = self.calculate_q_value_loss(\n",
    "            q1_current=q1_current,\n",
    "            q2_current=q2_current,\n",
    "            target_q_values=target_q_values,\n",
    "            rewards=rewards_batch,\n",
    "            done=done_batch,\n",
    "            gamma=self.discount_factor_gamma,\n",
    "        )\n",
    "\n",
    "        # Update critic network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        q_value_loss.backward()  # type:ignore\n",
    "        self.critic_optimizer.step()  # type:ignore\n",
    "\n",
    "        # 2. Policy Loss Estimation and Optimization\n",
    "        action_probs, log_pis, min_q_values = self.estimate_policy_values(states_batch)\n",
    "        policy_loss = self.calculate_policy_loss(action_probs, log_pis, min_q_values)\n",
    "\n",
    "        # Update policy network\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()  # type:ignore\n",
    "        self.policy_optimizer.step()  # type:ignore\n",
    "\n",
    "        # 3. Soft Update for Target Network\n",
    "        with torch.no_grad():\n",
    "            soft_update(self.target_critic_network, self.critic_network, self.soft_update_rate_tau)\n",
    "\n",
    "        return policy_loss.item(), q_value_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (q1): MLP(\n",
       "    (fc_1): Linear(in_features=12, out_features=128, bias=True)\n",
       "    (fc_2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (q2): MLP(\n",
       "    (fc_1): Linear(in_features=12, out_features=128, bias=True)\n",
       "    (fc_2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Policy\n",
    "policy_network = PolicyNetwork(n_observations, n_actions)\n",
    "\n",
    "# Q Network Q1 and Q2\n",
    "q_network = QNetwork(n_observations, n_actions)\n",
    "\n",
    "# Target Q Network Q1_target and Q2_target\n",
    "q_network_target = QNetwork(n_observations, n_actions)\n",
    "\n",
    "# Hard update target initially so that Q networks have same weights in the beginning\n",
    "hard_update(q_network_target, q_network)\n",
    "\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "# Send all NNs to device you have available and want to use\n",
    "policy_network.to(device)\n",
    "q_network.to(device)\n",
    "q_network_target.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor_gamma = 0.99\n",
    "soft_update_rate_tau = 0.05\n",
    "exploration_temperature_alpha = 0.2\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_actor_critic_agent = SoftActorCriticAgent(\n",
    "    actor_network=policy_network,\n",
    "    critic_network=q_network,\n",
    "    target_critic_network=q_network_target,\n",
    "    discount_factor_gamma=discount_factor_gamma,\n",
    "    soft_update_rate_tau=soft_update_rate_tau,\n",
    "    exploration_temperature_alpha=exploration_temperature_alpha,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "# Fill replay memory to minimum\n",
    "for i in range(batch_size):\n",
    "    state, _ = env.reset()\n",
    "    action = 0\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # Store the experience in replay memory\n",
    "    replay_memory.push(state, action, reward, next_state, terminated or truncated)\n",
    "\n",
    "print(len(replay_memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ActorCriticAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_policy_on_batch\u001b[39m(env: gym\u001b[38;5;241m.\u001b[39mEnv, agent: \u001b[43mActorCriticAgent\u001b[49m, device: torch\u001b[38;5;241m.\u001b[39mdevice, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    Collects data from multiple episodes and trains the policy on the combined batch.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m        dict: A dictionary of metrics tracking batch performance and training progress.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m global_track_steps_per_episode, global_track_reward_per_episode\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ActorCriticAgent' is not defined"
     ]
    }
   ],
   "source": [
    "def record_data_and_train_on_a_batch(\n",
    "    env: gym.Env,\n",
    "    agent: SoftActorCriticAgent,\n",
    "    device: torch.device,\n",
    "    batch_size: int = 256,\n",
    "    updates_per_episode_recorded: int = 5,\n",
    ") -> dict:\n",
    "    global global_track_steps_per_episode, global_track_reward_per_episode\n",
    "\n",
    "    # Lists to store batch data\n",
    "    batch_log_probs = []\n",
    "    batch_rewards = []\n",
    "    batch_returns = []\n",
    "    batch_values = []\n",
    "\n",
    "    total_rewards = []\n",
    "    total_steps = 0\n",
    "\n",
    "    # Reset the environment for a new episode\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = 0\n",
    "    episode_rewards = []\n",
    "\n",
    "    while True:\n",
    "        # Convert state to tensor and send it to the device\n",
    "        state_tensor = torch.Tensor(state).to(device)\n",
    "\n",
    "        # Select an action using the policy\n",
    "        action, log_prob = agent.sample_stochastic_action_with_log_prob(state=state_tensor)\n",
    "\n",
    "        # Take the selected action in the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        episode_steps += 1\n",
    "\n",
    "        # Store log-probability and reward\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        # Update cumulative reward and state\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        total_steps += 1\n",
    "\n",
    "        # Break if the episode ends\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    global_track_steps_per_episode.append(episode_steps)\n",
    "    global_track_reward_per_episode.append(episode_reward)\n",
    "\n",
    "    plot_metrics(global_track_steps_per_episode, global_track_reward_per_episode)\n",
    "\n",
    "    for i in range(updates_per_episode_recorded):\n",
    "        # Sample a batch\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_signal_batch = replay_memory.sample(batch_size)\n",
    "\n",
    "        # Convert numpy arrays to PyTorch tensors and move to specified device (CPU/GPU)\n",
    "        state_batch = torch.FloatTensor(state_batch).to(device)  # Current states\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(device)  # Next states\n",
    "        action_batch = torch.FloatTensor(action_batch).to(device)  # Actions taken\n",
    "        reward_batch = torch.FloatTensor(reward_batch).to(device).unsqueeze(1)  # Rewards received\n",
    "        done_signal_batch = torch.FloatTensor(done_signal_batch).to(device).unsqueeze(1)  # Episode termination signals\n",
    "\n",
    "        q1_current, q2_current, target_q_values = agent.estimate_q_values()\n",
    "\n",
    "        \n",
    "    # Compute discounted returns for the episode and store them\n",
    "    episode_returns = agent.compute_discounted_returns(torch.Tensor(episode_rewards))\n",
    "    batch_returns.extend(episode_returns)\n",
    "\n",
    "    # Convert batch data to tensors\n",
    "    batch_log_probs_tensor = torch.stack(batch_log_probs).to(device)\n",
    "    batch_returns_tensor = torch.Tensor(batch_returns).to(device)\n",
    "    batch_values_tensor = torch.stack(batch_values).to(device)\n",
    "\n",
    "    # Optimize the policy\n",
    "    policy_loss, critic_loss = agent.optimize(batch_log_probs_tensor, batch_returns_tensor, batch_values_tensor)\n",
    "\n",
    "    # Return metrics to track training progress\n",
    "    metrics = {\n",
    "        \"batch_reward\": sum(total_rewards) / batch_size,  # Average reward per episode\n",
    "        \"total_steps\": total_steps // batch_size,\n",
    "        \"policy_loss\": policy_loss,\n",
    "        \"critic_loss\": critic_loss,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vpg_policy(\n",
    "    max_episodes: int = 500, reward_threshold: float = 200.0, rolling_window: int = 50, batch_size: int = 16\n",
    "):\n",
    "    rewards_history = []\n",
    "    loss_history = []\n",
    "    steps_history = []\n",
    "\n",
    "    best_average_reward = 0\n",
    "\n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        metrics = train_policy_on_batch(env, actor_critic_agent, device, batch_size)\n",
    "\n",
    "        # Collect metrics\n",
    "        rewards_history.append(metrics[\"batch_reward\"])\n",
    "        loss_history.append(metrics[\"policy_loss\"])\n",
    "        steps_history.append(metrics[\"total_steps\"])\n",
    "\n",
    "        # Print metrics every 50 episodes\n",
    "        avg_reward = np.mean(rewards_history[-50:])\n",
    "\n",
    "        if episode % 5 == 0:\n",
    "            print(\n",
    "                f\"Episode {episode}: Average Reward: {avg_reward:.2f}, \"\n",
    "                f\"Loss: {metrics['policy_loss']:.4f}, Steps: {metrics['total_steps']}\"\n",
    "            )\n",
    "\n",
    "        # Convergence condition: Check if the rolling average exceeds the reward threshold\n",
    "        if len(rewards_history) >= rolling_window:\n",
    "            avg_rolling_reward = np.mean(rewards_history[-rolling_window:])\n",
    "            if avg_rolling_reward >= reward_threshold:\n",
    "                print(\n",
    "                    f\"Environment solved in {episode} episodes! \"\n",
    "                    f\"Average reward over the last {rolling_window} episodes: {avg_rolling_reward:.2f}\"\n",
    "                )\n",
    "                break\n",
    "        if avg_reward >= best_average_reward:\n",
    "            best_average_reward = avg_reward\n",
    "\n",
    "            # Create a filename that includes both the steps_done and timestamp\n",
    "            filename = f\"output/actor_critic_policy_network_lunar_lander_v3_bs_{batch_size}_{timestamp}.pth\"\n",
    "\n",
    "            # Save the policy network with the dynamically generated filename\n",
    "            torch.save(policy.policy_network.state_dict(), filename)\n",
    "\n",
    "            print(f\"Episode: {episode}, Average reward: {avg_reward}. Model saved as: {filename}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return rewards_history, loss_history, steps_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 100\n",
    "reward_threshold = 200.0\n",
    "batch_size = 16\n",
    "\n",
    "global_track_steps_per_episode = []\n",
    "global_track_reward_per_episode = []\n",
    "\n",
    "rewards_history, loss_history, steps_history = train_vpg_policy(\n",
    "    max_episodes=max_episodes, reward_threshold=reward_threshold, batch_size=batch_size\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
