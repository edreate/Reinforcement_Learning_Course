{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic\n",
    "\n",
    "#### Off Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Lunar Lander v3 as environment\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = int(env.action_space.n)\n",
    "print(f\"Number of possible actions: {n_actions}\")\n",
    "print(\"\"\"Actions:\n",
    "0: do nothing\n",
    "1: fire left orientation engine\n",
    "2: fire main engine\n",
    "3: fire right orientation engine\n",
    "\"\"\")\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(f\"Number of state observations: {n_observations}\")\n",
    "\n",
    "print(\"\"\"State (Observation Space):\n",
    "x, y\n",
    "vel_x, vel_y\n",
    "angle, angle_vel\n",
    "left_leg_touching, right_leg_touching\n",
    "      \"\"\")\n",
    "print(\"Current state: \", state)\n",
    "\n",
    "print(\"\"\"Units of the state are as follows:\n",
    "      ‘x’: (units), ‘y’: (units), \n",
    "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
    "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment to get the initial state\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(50):\n",
    "    env.step(action=0)\n",
    "# Render the environment to get an RGB image\n",
    "frame = env.render()\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Lunar Lander Environment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def plot_metrics(episode_durations, rewards, show_result=False, save_path=None):\n",
    "    # Create a horizontal figure with two subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5), dpi=100)\n",
    "    fig.suptitle(\"Training Metrics\" if not show_result else \"Results\", fontsize=16)\n",
    "\n",
    "    # Plot Episode Durations\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    axes[0].set_title(\"Episode Durations\")\n",
    "    axes[0].set_xlabel(\"Episode\")\n",
    "    axes[0].set_ylabel(\"Duration\")\n",
    "    axes[0].plot(durations_t.numpy(), label=\"Duration\")\n",
    "\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        axes[0].plot(means.numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot Rewards\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float)\n",
    "    axes[1].set_title(\"Rewards\")\n",
    "    axes[1].set_xlabel(\"Episode\")\n",
    "    axes[1].set_ylabel(\"Reward\")\n",
    "    axes[1].plot(rewards_t.numpy(), label=\"Reward\")\n",
    "\n",
    "    if len(rewards_t) >= 100:\n",
    "        reward_means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        reward_means = torch.cat((torch.zeros(99), reward_means))\n",
    "        axes[1].plot(reward_means.numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Adjust layout and save/show\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add space for the suptitle\n",
    "    if save_path:\n",
    "        plt.savefig(save_path + \".png\", dpi=300)\n",
    "        print(f\"Metrics figure saved to {save_path}\")\n",
    "\n",
    "    if \"get_ipython\" in globals():\n",
    "        if not show_result:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(fig)\n",
    "        else:\n",
    "            display.display(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from typing import List, Deque\n",
    "import random\n",
    "\n",
    "# Define the type of the Transition tuple\n",
    "Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\", \"done\"))\n",
    "\n",
    "\n",
    "# ReplayMemory class with strong type hints\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity: int):\n",
    "        # The deque stores Transition objects\n",
    "        self.memory: Deque[Transition] = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, state: float, action: int, next_state: float, reward: float, done: bool) -> None:\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(state, action, next_state, reward, int(done)))\n",
    "\n",
    "    def sample(self, batch_size: int) -> List[Transition]:\n",
    "        \"\"\"Sample a batch of transitions\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the current size of the memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from models import MLP\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Weight Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(target: nn.Module, source: nn.Module, tau: float) -> None:\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "\n",
    "def hard_update(target: nn.Module, source: nn.Module) -> None:\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for policy-based reinforcement learning.\n",
    "\n",
    "    Architecture:\n",
    "    - Input layer: Accepts `num_inputs` features representing the state.\n",
    "    - Hidden layers: Two fully connected layers with 256 units each and ReLU activation for non-linearity.\n",
    "    - Output layer: Produces `num_outputs`, representing action space size or logits.\n",
    "\n",
    "    Args:\n",
    "        num_inputs (int): Number of input features (state size).\n",
    "        num_outputs (int): Number of output features (action size).\n",
    "\n",
    "    Methods:\n",
    "        forward(x): Propagates the input through the network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs: int, num_outputs: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 256), nn.ReLU(), nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, num_inputs).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, num_outputs).\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, num_observations: int, num_actions: int):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.q1 = MLP(input_dim=num_observations, output_dim=num_actions)\n",
    "        self.q2 = MLP(input_dim=num_observations, output_dim=num_actions)\n",
    "\n",
    "    def forward(self, state: torch.Tensor):\n",
    "        x1 = self.q1(state)\n",
    "        x2 = self.q2(state)\n",
    "\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftActorCriticAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        actor_network: nn.Module,\n",
    "        critic_network: nn.Module,\n",
    "        target_critic_network: nn.Module,\n",
    "        discount_factor_gamma: float = 0.99,\n",
    "        soft_update_rate_tau: float = 0.05,\n",
    "        exploration_temperature_alpha: float = 0.2,\n",
    "        learning_rate: float = 3e-3,\n",
    "    ) -> None:\n",
    "        self.discount_factor_gamma = discount_factor_gamma\n",
    "        self.soft_update_rate_tau = soft_update_rate_tau\n",
    "        self.exploration_temperature_alpha = exploration_temperature_alpha\n",
    "\n",
    "        self.actor_network = actor_network\n",
    "        self.critic_network = critic_network\n",
    "        self.target_critic_network = target_critic_network\n",
    "\n",
    "        self.policy_optimizer = Adam(self.actor_network.parameters(), lr=learning_rate)\n",
    "        self.critic_optimizer = Adam(self.critic_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def sample_stochastic_action_with_log_prob(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Stochastically selects an action and computes its log-probability for discrete SAC.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, torch.Tensor]:\n",
    "                - action (int): The chosen action index.\n",
    "                - log_prob (torch.Tensor): The log-probability of the chosen action.\n",
    "        \"\"\"\n",
    "        action_logits = self.actor_network(state)  # Get logits from the policy network\n",
    "        action_distribution = Categorical(logits=action_logits)  # Create a categorical distribution\n",
    "        sampled_action = action_distribution.sample()  # Sample an action\n",
    "        log_prob = action_distribution.log_prob(sampled_action)  # Compute log-probability\n",
    "        return int(sampled_action.item()), log_prob\n",
    "\n",
    "    def sample_greedy_action(self, state: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Selects the most likely action deterministically for evaluation.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state.\n",
    "\n",
    "        Returns:\n",
    "            int: The action index with the highest probability.\n",
    "        \"\"\"\n",
    "        action_logits = self.actor_network(state)  # Get logits from the policy network\n",
    "        greedy_action = torch.argmax(action_logits).item()  # Select the action with the highest logit\n",
    "        return int(greedy_action)\n",
    "\n",
    "    def estimate_policy_values(self, states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Estimates policy values: action probabilities, log probabilities, and minimum Q-values.\n",
    "\n",
    "        Categorical.probs (Tensor): event probabilities\n",
    "\n",
    "        Categorical.logits (Tensor): event log probabilities (unnormalized)\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): Batch of states for estimation.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "                - Action probabilities\n",
    "                - Log probabilities of actions\n",
    "                - Minimum Q-values from the critics\n",
    "        \"\"\"\n",
    "        # Get action logits and distribution\n",
    "        action_logits = self.actor_network(states)\n",
    "        action_distribution = Categorical(logits=action_logits)\n",
    "        action_probs = action_distribution.probs  # Action probabilities\n",
    "        log_pis = action_distribution.logits  # Log-probabilities\n",
    "\n",
    "        # Evaluate Q-values for all actions\n",
    "        q1_values, q2_values = self.critic_network(states)\n",
    "        min_q_values = torch.min(q1_values, q2_values)  # Minimum Q-values\n",
    "\n",
    "        return action_probs, log_pis, min_q_values\n",
    "\n",
    "    def calculate_policy_loss(\n",
    "        self, action_probs: torch.Tensor, log_pis: torch.Tensor, min_q_values: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the policy loss.\n",
    "\n",
    "        Args:\n",
    "            action_probs (torch.Tensor): Action probabilities.\n",
    "            log_pis (torch.Tensor): Log probabilities of actions.\n",
    "            min_q_values (torch.Tensor): Minimum Q-values from critics.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Mean policy loss.\n",
    "        #### Mathematical Equation:\n",
    "        For a state $$s$$, the policy loss is:$$\n",
    "        \\mathcal{L}_{\\pi} = \\mathbb{E}_{s \\sim D}\\left[ \\sum_{a} \\pi(a|s) \\left( \\alpha \\log \\pi(a|s) - Q(s, a) \\right) \\right]\n",
    "        $$\n",
    "        \"\"\"\n",
    "        # Weighted loss computation\n",
    "        weighted_loss = (action_probs * (self.exploration_temperature_alpha * log_pis - min_q_values)).sum(dim=1)\n",
    "        policy_loss = weighted_loss.mean()\n",
    "\n",
    "        return policy_loss\n",
    "\n",
    "    def estimate_q_values(\n",
    "        self, states: torch.Tensor, actions: torch.Tensor, next_states: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Estimates current and target Q-values.\n",
    "\n",
    "        Args:\n",
    "            states (torch.Tensor): Current states.\n",
    "            actions (torch.Tensor): Actions taken in the current states.\n",
    "            next_states (torch.Tensor): Next states.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Current Q-values and target Q-values.\n",
    "        \"\"\"\n",
    "        # Current Q-values\n",
    "        q1_values, q2_values = self.critic_network(states)\n",
    "        q1_current = q1_values.gather(1, actions).squeeze(1)\n",
    "        q2_current = q2_values.gather(1, actions).squeeze(1)\n",
    "\n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_action_logits = self.actor_network(next_states)\n",
    "            next_action_distribution = Categorical(logits=next_action_logits)\n",
    "            next_action_probs = next_action_distribution.probs  # type:ignore\n",
    "            next_action_log_probs = next_action_distribution.logits  # type:ignore\n",
    "\n",
    "            next_q1_values, next_q2_values = self.critic_network(next_states)\n",
    "            next_q_values = torch.min(next_q1_values, next_q2_values)  # type:ignore\n",
    "\n",
    "            target_q_values = torch.sum(\n",
    "                next_action_probs * (next_q_values - self.exploration_temperature_alpha * next_action_log_probs), dim=1\n",
    "            )\n",
    "        return q1_current, q2_current, target_q_values\n",
    "\n",
    "    def calculate_q_value_loss(\n",
    "        self,\n",
    "        q1_current: torch.Tensor,\n",
    "        q2_current: torch.Tensor,\n",
    "        target_q_values: torch.Tensor,\n",
    "        rewards: torch.Tensor,\n",
    "        done: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the Q-value loss.\n",
    "\n",
    "        Args:\n",
    "            q1_current (torch.Tensor): Q1 values for current actions.\n",
    "            q2_current (torch.Tensor): Q2 values for current actions.\n",
    "            target_q_values (torch.Tensor): Target Q-values.\n",
    "            rewards (torch.Tensor): Rewards received after taking actions.\n",
    "            done (torch.Tensor): Terminal flags for the episodes.\n",
    "            gamma (float): Discount factor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Combined Q-value loss for both critics.\n",
    "        #### Mathematical Equation:\n",
    "        For a state $$s$$, action $$a$$, reward $$r$$, next state $$s'$$, and discount factor $$\\gamma$$, the Q-value loss is:$$\n",
    "        \\mathcal{L}_Q = \\mathbb{E}_{(s, a, r, s') \\sim D}\\left[ \\left(Q_{\\theta_1}(s, a) - y\\right)^2 + \\left(Q_{\\theta_2}(s, a) - y\\right)^2 \\right]\n",
    "        $$\n",
    "\n",
    "        where:\n",
    "        $$\n",
    "        y = r + \\gamma (1 - \\text{done}) \\mathbb{E}_{a' \\sim \\pi}\\left[ Q(s', a') - \\alpha \\log \\pi(a'|s') \\right]\n",
    "        $$\n",
    "        \"\"\"\n",
    "        # Target Q-value for Bellman update\n",
    "        q_value_target = rewards + self.discount_factor_gamma * (1 - done) * target_q_values\n",
    "\n",
    "        # MSE loss\n",
    "        qf1_loss = F.mse_loss(q1_current, q_value_target)\n",
    "        qf2_loss = F.mse_loss(q2_current, q_value_target)\n",
    "\n",
    "        # Combined loss\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "        return qf_loss\n",
    "\n",
    "    def optimize(\n",
    "        self,\n",
    "        states_batch: torch.Tensor,\n",
    "        actions_batch: torch.Tensor,\n",
    "        rewards_batch: torch.Tensor,\n",
    "        next_states_batch: torch.Tensor,\n",
    "        done_batch: torch.Tensor,\n",
    "    ) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Performs an optimization step for the policy and critic networks using data from the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            states, actions, rewards, next_states, and done.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Policy loss and Q-value loss as Python floats.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Q-Value Loss Estimation and Optimization\n",
    "        q1_current, q2_current, target_q_values = self.estimate_q_values(\n",
    "            states=states_batch,\n",
    "            actions=actions_batch,\n",
    "            next_states=next_states_batch,\n",
    "        )\n",
    "\n",
    "        q_value_loss = self.calculate_q_value_loss(\n",
    "            q1_current=q1_current,\n",
    "            q2_current=q2_current,\n",
    "            target_q_values=target_q_values,\n",
    "            rewards=rewards_batch,\n",
    "            done=done_batch,\n",
    "        )\n",
    "\n",
    "        # Update critic network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        q_value_loss.backward()  # type:ignore\n",
    "        self.critic_optimizer.step()  # type:ignore\n",
    "\n",
    "        # 2. Policy Loss Estimation and Optimization\n",
    "        action_probs, log_pis, min_q_values = self.estimate_policy_values(states_batch)\n",
    "        policy_loss = self.calculate_policy_loss(action_probs, log_pis, min_q_values)\n",
    "\n",
    "        # Update policy network\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()  # type:ignore\n",
    "        self.policy_optimizer.step()  # type:ignore\n",
    "\n",
    "        # 3. Soft Update for Target Network\n",
    "        with torch.no_grad():\n",
    "            soft_update(self.target_critic_network, self.critic_network, self.soft_update_rate_tau)\n",
    "\n",
    "        return policy_loss.item(), q_value_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy\n",
    "policy_network = PolicyNetwork(n_observations, n_actions)\n",
    "\n",
    "# Q Network Q1 and Q2\n",
    "q_network = QNetwork(n_observations, n_actions)\n",
    "\n",
    "# Target Q Network Q1_target and Q2_target\n",
    "q_network_target = QNetwork(n_observations, n_actions)\n",
    "\n",
    "# Hard update target initially so that Q networks have same weights in the beginning\n",
    "hard_update(q_network_target, q_network)\n",
    "\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Send all NNs to device you have available and want to use\n",
    "policy_network.to(device)\n",
    "q_network.to(device)\n",
    "q_network_target.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor_gamma = 0.99\n",
    "soft_update_rate_tau = 0.05\n",
    "exploration_temperature_alpha = 0.2\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_actor_critic_agent = SoftActorCriticAgent(\n",
    "    actor_network=policy_network,\n",
    "    critic_network=q_network,\n",
    "    target_critic_network=q_network_target,\n",
    "    discount_factor_gamma=discount_factor_gamma,\n",
    "    soft_update_rate_tau=soft_update_rate_tau,\n",
    "    exploration_temperature_alpha=exploration_temperature_alpha,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill replay memory to minimum\n",
    "for i in range(batch_size):\n",
    "    state, _ = env.reset()\n",
    "    action = 0\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # Store the experience in replay memory\n",
    "    replay_memory.push(state, action, next_state, reward, terminated or truncated)\n",
    "\n",
    "print(len(replay_memory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def record_data_and_train_on_a_batch(\n",
    "#     env: gym.Env,\n",
    "#     agent: SoftActorCriticAgent,\n",
    "#     device: torch.device,\n",
    "#     batch_size: int = 256,\n",
    "#     updates_per_episode_recorded: int = 5,\n",
    "# ) -> dict:\n",
    "#     global global_track_steps_per_episode, global_track_reward_per_episode\n",
    "\n",
    "#     total_rewards = []\n",
    "#     total_steps = 0\n",
    "\n",
    "#     # Reset the environment for a new episode\n",
    "#     state, _ = env.reset()\n",
    "\n",
    "#     episode_reward = 0.0\n",
    "#     episode_steps = 0\n",
    "#     episode_rewards = []\n",
    "\n",
    "#     while True:\n",
    "#         # Convert state to tensor and send it to the device\n",
    "#         state_tensor = torch.Tensor(state).to(device)\n",
    "\n",
    "#         # Select an action using the policy\n",
    "#         action, log_prob = agent.sample_stochastic_action_with_log_prob(state=state_tensor)\n",
    "\n",
    "#         # Take the selected action in the environment\n",
    "#         next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "#         episode_steps += 1\n",
    "\n",
    "#         # Store log-probability and reward\n",
    "#         episode_rewards.append(reward)\n",
    "\n",
    "#         # Update cumulative reward and state\n",
    "#         episode_reward += reward\n",
    "#         state = next_state\n",
    "#         total_steps += 1\n",
    "\n",
    "#         # Break if the episode ends\n",
    "#         if terminated or truncated:\n",
    "#             break\n",
    "\n",
    "#     global_track_steps_per_episode.append(episode_steps)\n",
    "#     global_track_reward_per_episode.append(episode_reward)\n",
    "\n",
    "#     plot_metrics(global_track_steps_per_episode, global_track_reward_per_episode)\n",
    "\n",
    "#     for i in range(updates_per_episode_recorded):\n",
    "#         # 1. Sample a batch\n",
    "#         state_batch, action_batch, reward_batch, next_state_batch, done_signal_batch = replay_memory.sample(batch_size)\n",
    "#         policy_loss, q_values_loss = agent.optimize(state_batch, action_batch, reward_batch, next_state_batch, done_signal_batch)\n",
    "\n",
    "#     # Return metrics to track training progress\n",
    "#     metrics = {\n",
    "#         \"batch_reward\": sum(total_rewards) / batch_size,  # Average reward per episode\n",
    "#         \"total_steps\": total_steps // batch_size,\n",
    "#         \"policy_loss\": policy_loss,\n",
    "#         \"q_value_loss\": q_values_loss,\n",
    "#     }\n",
    "\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Optional, List\n",
    "\n",
    "\n",
    "def record_data_and_train_on_a_batch(\n",
    "    env: gym.Env,\n",
    "    agent: SoftActorCriticAgent,\n",
    "    replay_memory: ReplayMemory,  # Ensure ReplayMemory is passed as a parameter\n",
    "    device: torch.device,\n",
    "    batch_size: int = 256,\n",
    "    updates_per_episode_recorded: int = 5,\n",
    ") -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Records data from an episode and performs training on sampled batches from replay memory.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The Gym environment.\n",
    "        agent (SoftActorCriticAgent): The SAC agent.\n",
    "        replay_memory (ReplayMemory): The replay memory buffer.\n",
    "        device (torch.device): The device to run computations on.\n",
    "        batch_size (int, optional): Number of samples per batch. Defaults to 256.\n",
    "        updates_per_episode_recorded (int, optional): Number of optimization steps per episode. Defaults to 5.\n",
    "        global_track_steps_per_episode (List[int], optional): Tracker for steps per episode.\n",
    "        global_track_reward_per_episode (List[float], optional): Tracker for rewards per episode.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Optional[float]]: Metrics for tracking training progress.\n",
    "    \"\"\"\n",
    "    global global_track_steps_per_episode, global_track_reward_per_episode\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = 0\n",
    "\n",
    "    # Reset the environment for a new episode\n",
    "    reset_result = env.reset()\n",
    "    state = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
    "\n",
    "    while True:\n",
    "        # Convert state to tensor and send it to the device\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "\n",
    "        # Select an action using the policy\n",
    "        with torch.no_grad():\n",
    "            action, log_prob = agent.sample_stochastic_action_with_log_prob(state=state_tensor)\n",
    "\n",
    "        # Convert action to numpy array for env.step\n",
    "        action_np = action.cpu().numpy() if isinstance(action, torch.Tensor) else action\n",
    "\n",
    "        # Take the selected action in the environment\n",
    "        state, reward, terminated, truncated, _ = env.step(action_np)\n",
    "\n",
    "        episode_steps += 1\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Store transition in replay memory\n",
    "        replay_memory.push(state, action_np, next_state, reward, terminated or truncated)\n",
    "\n",
    "        \"state\", \"action\", \"next_state\", \"reward\", \"done\"\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Break if the episode ends\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Update global trackers\n",
    "    global_track_steps_per_episode.append(episode_steps)\n",
    "    global_track_reward_per_episode.append(episode_reward)\n",
    "\n",
    "    # Plot metrics (ensure plot_metrics is defined elsewhere)\n",
    "    plot_metrics(global_track_steps_per_episode, global_track_reward_per_episode)\n",
    "\n",
    "    policy_loss_over_number_of_updates = 0.0\n",
    "    q_value_loss_over_number_of_updates = 0.0\n",
    "\n",
    "    # Perform optimization steps\n",
    "    for _ in range(updates_per_episode_recorded):\n",
    "        if len(replay_memory) < batch_size:\n",
    "            break  # Not enough samples to train\n",
    "\n",
    "        # Sample a batch\n",
    "        transitions = replay_memory.sample(batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Helper function to convert a list of elements to a tensor\n",
    "        def to_tensor(element_list, dtype=torch.float32):\n",
    "            return torch.tensor(element_list, dtype=dtype, device=device)\n",
    "\n",
    "        # Convert each component of the batch to tensors\n",
    "        # Assuming states and next_states are already in a suitable format (e.g., numpy arrays or lists)\n",
    "        state_batch = torch.stack([to_tensor(state) for state in batch.state])\n",
    "        action_batch = torch.stack([to_tensor(action, dtype=torch.int64) for action in batch.action]).unsqueeze(1)\n",
    "        reward_batch = torch.stack([to_tensor(reward) for reward in batch.reward]).unsqueeze(\n",
    "            1\n",
    "        )  # Shape: (batch_size, 1)\n",
    "        next_state_batch = torch.stack([to_tensor(next_state) for next in batch.next_state])\n",
    "        done_batch = to_tensor(batch.done, dtype=torch.float32).unsqueeze(1)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Optimize the agent and retrieve losses\n",
    "        policy_loss, q_value_loss = agent.optimize(\n",
    "            state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "        )\n",
    "\n",
    "        policy_loss_over_number_of_updates += policy_loss\n",
    "        q_value_loss_over_number_of_updates += q_value_loss\n",
    "\n",
    "    # Calculate average reward per step\n",
    "    average_reward = episode_reward / episode_steps if episode_steps > 0 else 0.0\n",
    "\n",
    "    # Compile metrics\n",
    "    metrics = {\n",
    "        \"average_reward\": average_reward,\n",
    "        \"total_steps\": episode_steps,\n",
    "        \"policy_loss\": policy_loss_over_number_of_updates / updates_per_episode_recorded,\n",
    "        \"q_value_loss\": q_value_loss_over_number_of_updates / updates_per_episode_recorded,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    env: gym.Env,\n",
    "    agent: SoftActorCriticAgent,\n",
    "    replay_memory: ReplayMemory,\n",
    "    device: torch.device,\n",
    "    max_episodes: int = 500,\n",
    "    reward_threshold: float = 200.0,\n",
    "    rolling_window: int = 50,\n",
    "    batch_size: int = 256,\n",
    "    updates_per_episode_recorded: int = 5,\n",
    "    save_dir: str = \"output\",\n",
    ") -> Tuple[List[float], List[float], List[int]]:\n",
    "    \"\"\"\n",
    "    Trains the Soft Actor-Critic agent in the given environment.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The Gym environment.\n",
    "        agent (SoftActorCriticAgent): The SAC agent.\n",
    "        replay_memory (ReplayMemory): The replay memory buffer.\n",
    "        device (torch.device): The device to run computations on.\n",
    "        max_episodes (int, optional): Maximum number of episodes to train. Defaults to 500.\n",
    "        reward_threshold (float, optional): Reward threshold for solving the environment. Defaults to 200.0.\n",
    "        rolling_window (int, optional): Number of episodes to consider for rolling average. Defaults to 50.\n",
    "        batch_size (int, optional): Number of samples per batch for training. Defaults to 256.\n",
    "        updates_per_episode_recorded (int, optional): Number of optimization steps per episode. Defaults to 5.\n",
    "        save_dir (str, optional): Directory to save the trained models. Defaults to \"output\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[float], List[float], List[int]]: Histories of rewards, losses, and steps per episode.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize trackers\n",
    "    rewards_history: List[float] = []\n",
    "    loss_history: List[float] = []\n",
    "    steps_history: List[int] = []\n",
    "\n",
    "    best_average_reward: float = -np.inf  # Initialize to negative infinity\n",
    "\n",
    "    # Initialize global trackers for metrics\n",
    "    global_track_steps_per_episode: List[int] = []\n",
    "    global_track_reward_per_episode: List[float] = []\n",
    "\n",
    "    # Generate a timestamp for model saving\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        # Record data and perform training\n",
    "        metrics = record_data_and_train_on_a_batch(\n",
    "            env=env,\n",
    "            agent=agent,\n",
    "            replay_memory=replay_memory,\n",
    "            device=device,\n",
    "            batch_size=batch_size,\n",
    "            updates_per_episode_recorded=updates_per_episode_recorded,\n",
    "        )\n",
    "\n",
    "        # Collect metrics\n",
    "        rewards_history.append(metrics.get(\"average_reward\", 0.0))\n",
    "        loss_history.append(metrics.get(\"policy_loss\", 0.0) or metrics.get(\"q_value_loss\", 0.0))\n",
    "        steps_history.append(metrics.get(\"total_steps\", 0))\n",
    "\n",
    "        # Calculate the average reward over the last 'rolling_window' episodes\n",
    "        if len(rewards_history) >= rolling_window:\n",
    "            avg_rolling_reward = np.mean(rewards_history[-rolling_window:])\n",
    "        else:\n",
    "            avg_rolling_reward = np.mean(rewards_history)\n",
    "\n",
    "        # Print metrics every 5 episodes\n",
    "        if episode % 5 == 0:\n",
    "            current_loss = metrics.get(\"policy_loss\", 0.0)\n",
    "            current_steps = metrics.get(\"total_steps\", 0)\n",
    "            print(\n",
    "                f\"Episode {episode}: Average Reward (last {min(rolling_window, len(rewards_history))} episodes): {avg_rolling_reward:.2f}, \"\n",
    "                f\"Policy Loss: {current_loss:.4f}, Steps: {current_steps}\"\n",
    "            )\n",
    "\n",
    "        # Convergence condition: Check if the rolling average exceeds the reward threshold\n",
    "        if len(rewards_history) >= rolling_window and avg_rolling_reward >= reward_threshold:\n",
    "            print(\n",
    "                f\"Environment solved in {episode} episodes! \"\n",
    "                f\"Average reward over the last {rolling_window} episodes: {avg_rolling_reward:.2f}\"\n",
    "            )\n",
    "            # Optionally, save the final model\n",
    "            final_filename = os.path.join(save_dir, f\"actor_critic_policy_final_{timestamp}.pth\")\n",
    "            torch.save(agent.policy_network.state_dict(), final_filename)\n",
    "            print(f\"Final model saved as: {final_filename}\")\n",
    "            break\n",
    "\n",
    "        # Save the model if the current average reward is the best so far\n",
    "        if avg_rolling_reward > best_average_reward:\n",
    "            best_average_reward = avg_rolling_reward\n",
    "\n",
    "            # Create a filename that includes the episode number and timestamp\n",
    "            filename = os.path.join(save_dir, f\"actor_critic_policy_ep_{episode}_bs_{batch_size}_{timestamp}.pth\")\n",
    "\n",
    "            # Save the policy network with the dynamically generated filename\n",
    "            torch.save(agent.actor_network.state_dict(), filename)\n",
    "\n",
    "            print(f\"Episode: {episode}, Average reward: {avg_rolling_reward:.2f}. Model saved as: {filename}\")\n",
    "\n",
    "        # Optionally, plot metrics periodically or at the end\n",
    "        # if episode % 50 == 0:\n",
    "        #     plot_metrics(global_track_steps_per_episode, global_track_reward_per_episode)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return rewards_history, loss_history, steps_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 1000\n",
    "reward_threshold = 200.0\n",
    "\n",
    "global_track_steps_per_episode = []\n",
    "global_track_reward_per_episode = []\n",
    "\n",
    "rewards_history, loss_history, steps_history = train(\n",
    "    max_episodes=max_episodes,\n",
    "    reward_threshold=reward_threshold,\n",
    "    batch_size=batch_size,\n",
    "    agent=soft_actor_critic_agent,\n",
    "    device=device,\n",
    "    replay_memory=replay_memory,\n",
    "    env=env,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
