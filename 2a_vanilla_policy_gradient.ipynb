{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions: 4\n",
      "Actions:\n",
      "0: do nothing\n",
      "1: fire left orientation engine\n",
      "2: fire main engine\n",
      "3: fire right orientation engine\n",
      "\n",
      "Number of state observations: 8\n",
      "State (Observation Space):\n",
      "x, y\n",
      "vel_x, vel_y\n",
      "angle, angle_vel\n",
      "left_leg_touching, right_leg_touching\n",
      "      \n",
      "Current state:  [ 0.00446291  1.4098121   0.45203632 -0.04925022 -0.00516469 -0.10239319\n",
      "  0.          0.        ]\n",
      "Units of the state are as follows:\n",
      "      ‘x’: (units), ‘y’: (units), \n",
      "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
      "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = int(env.action_space.n)\n",
    "print(f\"Number of possible actions: {n_actions}\")\n",
    "print(\"\"\"Actions:\n",
    "0: do nothing\n",
    "1: fire left orientation engine\n",
    "2: fire main engine\n",
    "3: fire right orientation engine\n",
    "\"\"\")\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(f\"Number of state observations: {n_observations}\")\n",
    "\n",
    "print(\"\"\"State (Observation Space):\n",
    "x, y\n",
    "vel_x, vel_y\n",
    "angle, angle_vel\n",
    "left_leg_touching, right_leg_touching\n",
    "      \"\"\")\n",
    "print(\"Current state: \", state)\n",
    "\n",
    "print(\"\"\"Units of the state are as follows:\n",
    "      ‘x’: (units), ‘y’: (units), \n",
    "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
    "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment to get the initial state\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(50):\n",
    "    env.step(action=0)\n",
    "# Render the environment to get an RGB image\n",
    "frame = env.render()\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Lunar Lander Environment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VPG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam, Optimizer\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12b2496f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs: int, num_outputs: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 256), nn.ReLU(), nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGPolicy:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int,\n",
    "        num_outputs: int,\n",
    "        device: torch.device,\n",
    "        learning_rate: float = 0.003,\n",
    "        discount_factor: float = 0.99,\n",
    "    ) -> None:\n",
    "        self.device = device\n",
    "\n",
    "        self.policy_network = MLP(num_inputs, num_outputs)\n",
    "        self.policy_network.to(self.device)\n",
    "\n",
    "        self.optimizer = Adam(params=self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "\n",
    "    def select_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state and computes its log-probability.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state represented as a tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, torch.Tensor]:\n",
    "                - action (int): The chosen action index.\n",
    "                - log_prob (torch.Tensor): The log-probability of the chosen action.\n",
    "        \"\"\"\n",
    "        # Forward pass through the policy network to get action logits.\n",
    "        action_logits = self.policy_network(state)\n",
    "\n",
    "        # Create a categorical distribution from the logits.\n",
    "        action_distribution = Categorical(logits=action_logits)\n",
    "\n",
    "        # Sample an action from the distribution.\n",
    "        sampled_action = action_distribution.sample()\n",
    "\n",
    "        # Convert the sampled action to a Python integer.\n",
    "        action_index = int(sampled_action.item())\n",
    "\n",
    "        # Calculate the log-probability of the selected action.\n",
    "        log_prob = action_distribution.log_prob(sampled_action)\n",
    "\n",
    "        return action_index, log_prob\n",
    "\n",
    "    def compute_discounted_returns(self, rewards: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes discounted returns for a sequence of rewards.\n",
    "\n",
    "        Args:\n",
    "            rewards (torch.Tensor): Rewards for the episode.\n",
    "            gamma (float): Discount factor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Discounted returns.\n",
    "        \"\"\"\n",
    "        discounted_returns = []\n",
    "        cumulative_return = 0.0\n",
    "        for reward in reversed(rewards):\n",
    "            cumulative_return = reward + self.gamma * cumulative_return\n",
    "            discounted_returns.insert(0, cumulative_return)\n",
    "        discounted_returns = torch.tensor(discounted_returns, dtype=torch.float32)\n",
    "        return (discounted_returns - discounted_returns.mean()) / (discounted_returns.std() + 1e-8)\n",
    "\n",
    "    def calculate_policy_loss(\n",
    "        self, episode_log_probability_actions: torch.Tensor, episode_action_rewards: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The loss is negated because most optimization libraries (like PyTorch) perform minimization,\n",
    "        while the policy gradient aims to maximize the objective J(θ).\n",
    "        \"\"\"\n",
    "        return -(episode_log_probability_actions * episode_action_rewards).mean()\n",
    "\n",
    "    def optimize_policy(self, episode_log_probability_actions: torch.Tensor, episode_action_rewards: torch.Tensor):\n",
    "        loss = self.calculate_policy_loss(episode_log_probability_actions, episode_action_rewards)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "discount_factor = 0.99\n",
    "\n",
    "policy = VPGPolicy(\n",
    "    num_inputs=n_observations,\n",
    "    num_outputs=n_actions,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device,\n",
    "    discount_factor=discount_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_on_batch(env: gym.Env, policy: VPGPolicy, device: torch.device, batch_size: int = 16) -> dict:\n",
    "    \"\"\"\n",
    "    Collects data from multiple episodes and trains the policy on the combined batch.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to interact with (following the OpenAI Gym interface).\n",
    "        policy: The policy object that defines action selection and optimization.\n",
    "        device: The device to run computations on (CPU/GPU).\n",
    "        batch_size: Number of episodes to collect before updating the policy.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of metrics tracking batch performance and training progress.\n",
    "    \"\"\"\n",
    "    # Lists to store batch data\n",
    "    batch_log_probs = []\n",
    "    batch_rewards = []\n",
    "    batch_returns = []\n",
    "    total_rewards = []  # To track episode rewards for logging\n",
    "    total_steps = 0  # To track steps taken across episodes\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        # Reset the environment for a new episode\n",
    "        state, _ = env.reset()\n",
    "        episode_log_probs = []\n",
    "        episode_rewards = []\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while True:\n",
    "            # Convert state to tensor and send it to the device\n",
    "            state_tensor = torch.Tensor(state).to(device)\n",
    "\n",
    "            # Select an action using the policy\n",
    "            action, log_prob = policy.select_action(state=state_tensor)\n",
    "\n",
    "            # Take the selected action in the environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Store log-probability and reward\n",
    "            episode_log_probs.append(log_prob)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # Update cumulative reward and state\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "\n",
    "            # Break if the episode ends\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Store episode data into batch data\n",
    "        total_rewards.append(episode_reward)\n",
    "        batch_log_probs.extend(episode_log_probs)\n",
    "        batch_rewards.extend(episode_rewards)\n",
    "\n",
    "        # Compute discounted returns for the episode and store them\n",
    "        episode_returns = policy.compute_discounted_returns(torch.Tensor(episode_rewards))\n",
    "        batch_returns.extend(episode_returns)\n",
    "\n",
    "    # Convert batch data to tensors\n",
    "    batch_log_probs_tensor = torch.stack(batch_log_probs).to(device)\n",
    "    batch_returns_tensor = torch.Tensor(batch_returns).to(device)\n",
    "\n",
    "    # Optimize the policy\n",
    "    policy_loss = policy.optimize_policy(batch_log_probs_tensor, batch_returns_tensor)\n",
    "\n",
    "    # Return metrics to track training progress\n",
    "    metrics = {\n",
    "        \"batch_reward\": sum(total_rewards) / batch_size,  # Average reward per episode\n",
    "        \"total_steps\": total_steps,\n",
    "        \"policy_loss\": policy_loss,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vpg_policy(\n",
    "    max_episodes: int = 500, reward_threshold: float = 200.0, rolling_window: int = 100, batch_size: int = 16\n",
    "):\n",
    "    rewards_history = []\n",
    "    loss_history = []\n",
    "    steps_history = []\n",
    "\n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        metrics = train_policy_on_batch(env, policy, device, batch_size)\n",
    "\n",
    "        # Collect metrics\n",
    "        rewards_history.append(metrics[\"batch_reward\"])\n",
    "        loss_history.append(metrics[\"policy_loss\"])\n",
    "        steps_history.append(metrics[\"total_steps\"])\n",
    "\n",
    "        # Print metrics every 50 episodes\n",
    "        if episode % 50 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-50:])\n",
    "            print(\n",
    "                f\"Episode {episode}: Average Reward: {avg_reward:.2f}, \"\n",
    "                f\"Loss: {metrics['policy_loss']:.4f}, Steps: {metrics['total_steps']}\"\n",
    "            )\n",
    "\n",
    "        # Convergence condition: Check if the rolling average exceeds the reward threshold\n",
    "        if len(rewards_history) >= rolling_window:\n",
    "            avg_rolling_reward = np.mean(rewards_history[-rolling_window:])\n",
    "            if avg_rolling_reward >= reward_threshold:\n",
    "                print(\n",
    "                    f\"Environment solved in {episode} episodes! \"\n",
    "                    f\"Average reward over the last {rolling_window} episodes: {avg_rolling_reward:.2f}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return rewards_history, loss_history, steps_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 100\n",
    "reward_threshold = 200.0\n",
    "batch_size = 16\n",
    "\n",
    "rewards_history, loss_history, steps_history = train_vpg_policy(\n",
    "    max_episodes=max_episodes, reward_threshold=reward_threshold, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rewards_history, label=\"Reward per Episode\")\n",
    "plt.axhline(y=reward_threshold, color=\"r\", linestyle=\"--\", label=\"Reward Threshold\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(loss_history, label=\"Loss per Episode\", color=\"orange\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Policy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot steps\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(steps_history, label=\"Steps per Episode\", color=\"green\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.title(\"Steps Taken\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
