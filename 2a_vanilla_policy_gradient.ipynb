{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions: 4\n",
      "Actions:\n",
      "0: do nothing\n",
      "1: fire left orientation engine\n",
      "2: fire main engine\n",
      "3: fire right orientation engine\n",
      "\n",
      "Number of state observations: 8\n",
      "State (Observation Space):\n",
      "x, y\n",
      "vel_x, vel_y\n",
      "angle, angle_vel\n",
      "left_leg_touching, right_leg_touching\n",
      "      \n",
      "Current state:  [ 0.00331793  1.4038675   0.3360535  -0.31345463 -0.00383786 -0.07612097\n",
      "  0.          0.        ]\n",
      "Units of the state are as follows:\n",
      "      ‘x’: (units), ‘y’: (units), \n",
      "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
      "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = int(env.action_space.n)\n",
    "print(f\"Number of possible actions: {n_actions}\")\n",
    "print(\"\"\"Actions:\n",
    "0: do nothing\n",
    "1: fire left orientation engine\n",
    "2: fire main engine\n",
    "3: fire right orientation engine\n",
    "\"\"\")\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(f\"Number of state observations: {n_observations}\")\n",
    "\n",
    "print(\"\"\"State (Observation Space):\n",
    "x, y\n",
    "vel_x, vel_y\n",
    "angle, angle_vel\n",
    "left_leg_touching, right_leg_touching\n",
    "      \"\"\")\n",
    "print(\"Current state: \", state)\n",
    "\n",
    "print(\"\"\"Units of the state are as follows:\n",
    "      ‘x’: (units), ‘y’: (units), \n",
    "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
    "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment to get the initial state\n",
    "state, info = env.reset()\n",
    "\n",
    "# Render the environment to get an RGB image\n",
    "frame = env.render()\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Lunar Lander Environment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VPG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam, Optimizer\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_inputs: int, num_outputs: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 256), nn.ReLU(), nn.Linear(256, 256), nn.ReLU(), nn.Linear(256, num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGPolicy:\n",
    "    def __init__(self, num_inputs: int, num_outputs: int, device: torch.device, learning_rate: float = 0.003) -> None:\n",
    "        self.device = device\n",
    "        self.policy_network = MLP(num_inputs, num_outputs)\n",
    "        self.policy_network.to(self.device)\n",
    "        self.optimizer = Adam(params=self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "    def select_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state and computes its log-probability.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state represented as a tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, torch.Tensor]:\n",
    "                - action (int): The chosen action index.\n",
    "                - log_prob (torch.Tensor): The log-probability of the chosen action.\n",
    "        \"\"\"\n",
    "        # Forward pass through the policy network to get action logits.\n",
    "        action_logits = self.policy_network(state)\n",
    "\n",
    "        # Create a categorical distribution from the logits.\n",
    "        action_distribution = Categorical(logits=action_logits)\n",
    "\n",
    "        # Sample an action from the distribution.\n",
    "        sampled_action = action_distribution.sample()\n",
    "\n",
    "        # Convert the sampled action to a Python integer.\n",
    "        action_index = int(sampled_action.item())\n",
    "\n",
    "        # Calculate the log-probability of the selected action.\n",
    "        log_prob = action_distribution.log_prob(sampled_action)\n",
    "\n",
    "        return action_index, log_prob\n",
    "\n",
    "    def calculate_policy_loss(\n",
    "        self, episode_log_probability_actions: torch.Tensor, episode_action_rewards: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The loss is negated because most optimization libraries (like PyTorch) perform minimization,\n",
    "        while the policy gradient aims to maximize the objective J(θ).\n",
    "        \"\"\"\n",
    "        return -(episode_log_probability_actions * episode_action_rewards).mean()\n",
    "\n",
    "    def optimize_policy(self, episode_log_probability_actions: torch.Tensor, episode_action_rewards: torch.Tensor):\n",
    "        loss = self.calculate_policy_loss(episode_log_probability_actions, episode_action_rewards)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# print(\"Using device: \", device)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "\n",
    "policy = VPGPolicy(num_inputs=n_observations, num_outputs=n_actions, learning_rate=learning_rate, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_on_one_episode(env: gym.Env, policy: VPGPolicy, device: torch.device) -> dict:\n",
    "    \"\"\"\n",
    "    Runs one episode of training for the policy in the given environment.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to interact with (following the OpenAI Gym interface).\n",
    "        policy: The policy object that defines action selection and optimization.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of metrics tracking episode performance and training progress.\n",
    "    \"\"\"\n",
    "    # Lists to store log-probabilities of actions and rewards for the episode\n",
    "    log_probs_list = []\n",
    "    rewards_list = []\n",
    "\n",
    "    # Initialize episode reward and step counter\n",
    "    total_episode_reward = 0.0\n",
    "    steps_taken = 0\n",
    "\n",
    "    # Reset the environment for a new episode\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # Convert state to a tensor\n",
    "        state_tensor = torch.Tensor(state)\n",
    "        state_tensor = state_tensor.to(device)\n",
    "\n",
    "        # Select an action using the policy and get its log-probability\n",
    "        action, log_prob = policy.select_action(state=state_tensor)\n",
    "\n",
    "        # Take the selected action in the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action=action)\n",
    "\n",
    "        # Store log-probability and reward\n",
    "        log_probs_list.append(log_prob)\n",
    "        rewards_list.append(reward)\n",
    "\n",
    "        # Update cumulative reward\n",
    "        total_episode_reward += reward\n",
    "\n",
    "        # Update state and step counter\n",
    "        state = next_state\n",
    "        steps_taken += 1\n",
    "\n",
    "        # Exit the loop if the episode ends\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Convert collected data to tensors\n",
    "    log_probs_tensor = torch.stack(log_probs_list)  # Log-probabilities for all actions taken\n",
    "    rewards_tensor = torch.Tensor(rewards_list)  # Rewards received during the episode\n",
    "\n",
    "    log_probs_tensor = log_probs_tensor.to(device)\n",
    "    rewards_tensor = rewards_tensor.to(device)\n",
    "\n",
    "    # Compute the return (discounted rewards) for policy optimization\n",
    "    # returns_tensor = compute_discounted_returns(rewards_tensor, gamma=policy.gamma)\n",
    "\n",
    "    # Optimize the policy\n",
    "    policy_loss = policy.optimize_policy(log_probs_tensor, rewards_tensor)\n",
    "\n",
    "    # Return metrics to track training progress\n",
    "    metrics = {\n",
    "        \"episode_reward\": total_episode_reward,\n",
    "        \"steps_taken\": steps_taken,\n",
    "        \"policy_loss\": policy_loss,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vpg_policy(max_episodes: int = 500):\n",
    "    \n",
    "    reward_threshold = 200  # Target for \"solved\" in Lunar Lander\n",
    "    rewards_history = []\n",
    "    loss_history = []\n",
    "    steps_history = []\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        metrics = train_policy_on_one_episode(env, policy, device)\n",
    "\n",
    "        # Collect metrics\n",
    "        rewards_history.append(metrics[\"episode_reward\"])\n",
    "        loss_history.append(metrics[\"policy_loss\"])\n",
    "        steps_history.append(metrics[\"steps_taken\"])\n",
    "\n",
    "        # Print metrics every 50 episodes\n",
    "        if episode % 50 == 0:\n",
    "            avg_reward = np.mean(rewards_history[-50:])\n",
    "            print(f\"Episode {episode}: Average Reward: {avg_reward}, Loss: {metrics['policy_loss'] : .4f}, Steps: {metrics['steps_taken']}\")\n",
    "\n",
    "        # Check convergence (solving condition)\n",
    "        if len(rewards_history) >= 100 and np.mean(rewards_history[-100:]) >= reward_threshold:\n",
    "            print(f\"Environment solved in {episode + 1} episodes!\")\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return rewards_history, loss_history, steps_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Average Reward: -413.4776692630029, Loss: -5.1396098136901855, Steps: 107\n",
      "Episode 50: Average Reward: -425.7804941291074, Loss: -1.2635306119918823, Steps: 105\n",
      "Episode 100: Average Reward: -188.79744238799515, Loss: -1.1577503681182861, Steps: 137\n",
      "Episode 150: Average Reward: -265.7070379793048, Loss: -0.6950559020042419, Steps: 98\n",
      "Episode 200: Average Reward: -290.61194520032774, Loss: -1.1474814414978027, Steps: 110\n",
      "Episode 250: Average Reward: -260.4633075766067, Loss: -0.9798973202705383, Steps: 121\n",
      "Episode 300: Average Reward: -226.3971887705259, Loss: -0.43734225630760193, Steps: 296\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rewards_history, loss_history, steps_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_vpg_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[101], line 9\u001b[0m, in \u001b[0;36mtrain_vpg_policy\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m steps_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_episodes):\n\u001b[0;32m----> 9\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_policy_on_one_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Collect metrics\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     rewards_history\u001b[38;5;241m.\u001b[39mappend(metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[98], line 29\u001b[0m, in \u001b[0;36mtrain_policy_on_one_episode\u001b[0;34m(env, policy, device)\u001b[0m\n\u001b[1;32m     26\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m state_tensor\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Select an action using the policy and get its log-probability\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Take the selected action in the environment\u001b[39;00m\n\u001b[1;32m     32\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m=\u001b[39maction)\n",
      "Cell \u001b[0;32mIn[95], line 33\u001b[0m, in \u001b[0;36mVPGPolicy.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     30\u001b[0m action_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sampled_action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate the log-probability of the selected action.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43maction_distribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action_index, log_prob\n",
      "File \u001b[0;32m~/Desktop/ft_productivity_helpers/rl-venv/lib/python3.9/site-packages/torch/distributions/categorical.py:140\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[1;32m    139\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m value, log_pmf \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log_pmf\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, value)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/ft_productivity_helpers/rl-venv/lib/python3.9/site-packages/torch/functional.py:77\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m(tensors)\n",
      "File \u001b[0;32m~/Desktop/ft_productivity_helpers/rl-venv/lib/python3.9/site-packages/torch/_VF.py:26\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards_history, loss_history, steps_history = train_vpg_policy(max_episodes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rewards_history, label=\"Reward per Episode\")\n",
    "plt.axhline(y=reward_threshold, color=\"r\", linestyle=\"--\", label=\"Reward Threshold\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(loss_history, label=\"Loss per Episode\", color=\"orange\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Policy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot steps\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(steps_history, label=\"Steps per Episode\", color=\"green\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.title(\"Steps Taken\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
