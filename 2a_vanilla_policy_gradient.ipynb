{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions: 4\n",
      "Actions:\n",
      "0: do nothing\n",
      "1: fire left orientation engine\n",
      "2: fire main engine\n",
      "3: fire right orientation engine\n",
      "\n",
      "Number of state observations: 8\n",
      "State (Observation Space):\n",
      "x, y\n",
      "vel_x, vel_y\n",
      "angle, angle_vel\n",
      "left_leg_touching, right_leg_touching\n",
      "      \n",
      "Current state:  [ 0.00691633  1.4223201   0.70054257  0.5066341  -0.0080076  -0.15868346\n",
      "  0.          0.        ]\n",
      "Units of the state are as follows:\n",
      "      ‘x’: (units), ‘y’: (units), \n",
      "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
      "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=False)\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = int(env.action_space.n)\n",
    "print(f\"Number of possible actions: {n_actions}\")\n",
    "print(\"\"\"Actions:\n",
    "0: do nothing\n",
    "1: fire left orientation engine\n",
    "2: fire main engine\n",
    "3: fire right orientation engine\n",
    "\"\"\")\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(f\"Number of state observations: {n_observations}\")\n",
    "\n",
    "print(\"\"\"State (Observation Space):\n",
    "x, y\n",
    "vel_x, vel_y\n",
    "angle, angle_vel\n",
    "left_leg_touching, right_leg_touching\n",
    "      \"\"\")\n",
    "print(\"Current state: \", state)\n",
    "\n",
    "print(\"\"\"Units of the state are as follows:\n",
    "      ‘x’: (units), ‘y’: (units), \n",
    "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
    "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edreate/Desktop/Reinforcement_Learning_Course/venv/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:672: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Image data of dtype object cannot be converted to float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Plot the image\u001b[39;00m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m---> 11\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunar Lander Environment\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Reinforcement_Learning_Course/venv/lib/python3.10/site-packages/matplotlib/pyplot.py:3562\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3543\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3560\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3561\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3562\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3566\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3567\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3568\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3571\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3573\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3577\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m     sci(__ret)\n\u001b[1;32m   3582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/Desktop/Reinforcement_Learning_Course/venv/lib/python3.10/site-packages/matplotlib/__init__.py:1476\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1476\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1477\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1478\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1479\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1481\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1482\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1483\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/Desktop/Reinforcement_Learning_Course/venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5895\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5895\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5896\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Reinforcement_Learning_Course/venv/lib/python3.10/site-packages/matplotlib/image.py:729\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    728\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Reinforcement_Learning_Course/venv/lib/python3.10/site-packages/matplotlib/image.py:692\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    690\u001b[0m A \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39msafe_masked_invalid(A, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcan_cast(A\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msame_kind\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage data of dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    693\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverted to float\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    695\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Reset the environment to get the initial state\n",
    "# state, info = env.reset()\n",
    "\n",
    "# for i in range(50):\n",
    "#     env.step(action=0)\n",
    "# # Render the environment to get an RGB image\n",
    "# frame = env.render()\n",
    "\n",
    "# # Plot the image\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.imshow(frame)\n",
    "# plt.axis(\"off\")\n",
    "# plt.title(\"Lunar Lander Environment\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VPG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam, Optimizer\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from models import PolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGPolicy:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int,\n",
    "        num_outputs: int,\n",
    "        device: torch.device,\n",
    "        learning_rate: float = 0.003,\n",
    "        discount_factor: float = 0.99,\n",
    "    ) -> None:\n",
    "        self.device = device\n",
    "\n",
    "        self.policy_network = PolicyNetwork(num_inputs, num_outputs)\n",
    "        self.policy_network.to(self.device)\n",
    "\n",
    "        self.optimizer = Adam(params=self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "\n",
    "    def select_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state and computes its log-probability.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state represented as a tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, torch.Tensor]:\n",
    "                - action (int): The chosen action index.\n",
    "                - log_prob (torch.Tensor): The log-probability of the chosen action.\n",
    "        \"\"\"\n",
    "        # Forward pass through the policy network to get action logits.\n",
    "        action_logits = self.policy_network(state)\n",
    "\n",
    "        # Create a categorical distribution from the logits.\n",
    "        action_distribution = Categorical(logits=action_logits)\n",
    "\n",
    "        # Sample an action from the distribution.\n",
    "        sampled_action = action_distribution.sample()\n",
    "\n",
    "        # Convert the sampled action to a Python integer.\n",
    "        action_index = int(sampled_action.item())\n",
    "\n",
    "        # Calculate the log-probability of the selected action.\n",
    "        log_prob = action_distribution.log_prob(sampled_action)\n",
    "\n",
    "        return action_index, log_prob\n",
    "\n",
    "    def compute_discounted_returns(self, rewards: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes discounted returns for a sequence of rewards.\n",
    "\n",
    "        Args:\n",
    "            rewards (torch.Tensor): Rewards for the episode.\n",
    "            gamma (float): Discount factor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Discounted returns.\n",
    "        \"\"\"\n",
    "        discounted_returns = []\n",
    "        cumulative_return = 0.0\n",
    "        for reward in reversed(rewards):\n",
    "            cumulative_return = reward + self.gamma * cumulative_return\n",
    "            discounted_returns.insert(0, cumulative_return)\n",
    "        discounted_returns = torch.tensor(discounted_returns, dtype=torch.float32)\n",
    "        return (discounted_returns - discounted_returns.mean()) / (discounted_returns.std() + 1e-8)\n",
    "\n",
    "    def calculate_policy_loss(\n",
    "        self, episode_log_probability_actions: torch.Tensor, episode_action_rewards: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The loss is negated because most optimization libraries (like PyTorch) perform minimization,\n",
    "        while the policy gradient aims to maximize the objective J(θ).\n",
    "        \"\"\"\n",
    "        return -(episode_log_probability_actions * episode_action_rewards).mean()\n",
    "\n",
    "    def optimize_policy(self, episode_log_probability_actions: torch.Tensor, episode_action_rewards: torch.Tensor):\n",
    "        loss = self.calculate_policy_loss(episode_log_probability_actions, episode_action_rewards)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "discount_factor = 0.99\n",
    "\n",
    "policy = VPGPolicy(\n",
    "    num_inputs=n_observations,\n",
    "    num_outputs=n_actions,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device,\n",
    "    discount_factor=discount_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_on_batch(env: gym.Env, policy: VPGPolicy, device: torch.device, batch_size: int = 16) -> dict:\n",
    "    \"\"\"\n",
    "    Collects data from multiple episodes and trains the policy on the combined batch.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to interact with (following the OpenAI Gym interface).\n",
    "        policy: The policy object that defines action selection and optimization.\n",
    "        device: The device to run computations on (CPU/GPU).\n",
    "        batch_size: Number of episodes to collect before updating the policy.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of metrics tracking batch performance and training progress.\n",
    "    \"\"\"\n",
    "    # Lists to store batch data\n",
    "    batch_log_probs = []\n",
    "    batch_rewards = []\n",
    "    batch_returns = []\n",
    "    total_rewards = []  # To track episode rewards for logging\n",
    "    total_steps = 0  # To track steps taken across episodes\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        # Reset the environment for a new episode\n",
    "        state, _ = env.reset()\n",
    "        episode_log_probs = []\n",
    "        episode_rewards = []\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while True:\n",
    "            # Convert state to tensor and send it to the device\n",
    "            state_tensor = torch.Tensor(state).to(device)\n",
    "\n",
    "            # Select an action using the policy\n",
    "            action, log_prob = policy.select_action(state=state_tensor)\n",
    "\n",
    "            # Take the selected action in the environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Store log-probability and reward\n",
    "            episode_log_probs.append(log_prob)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # Update cumulative reward and state\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "\n",
    "            # Break if the episode ends\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Store episode data into batch data\n",
    "        total_rewards.append(episode_reward)\n",
    "        batch_log_probs.extend(episode_log_probs)\n",
    "        batch_rewards.extend(episode_rewards)\n",
    "\n",
    "        # Compute discounted returns for the episode and store them\n",
    "        episode_returns = policy.compute_discounted_returns(torch.Tensor(episode_rewards))\n",
    "        batch_returns.extend(episode_returns)\n",
    "\n",
    "    # Convert batch data to tensors\n",
    "    batch_log_probs_tensor = torch.stack(batch_log_probs).to(device)\n",
    "    batch_returns_tensor = torch.Tensor(batch_returns).to(device)\n",
    "\n",
    "    # Optimize the policy\n",
    "    policy_loss = policy.optimize_policy(batch_log_probs_tensor, batch_returns_tensor)\n",
    "\n",
    "    # Return metrics to track training progress\n",
    "    metrics = {\n",
    "        \"batch_reward\": sum(total_rewards) / batch_size,  # Average reward per episode\n",
    "        \"total_steps\": total_steps // batch_size,\n",
    "        \"policy_loss\": policy_loss,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vpg_policy(\n",
    "    max_episodes: int = 500, reward_threshold: float = 200.0, rolling_window: int = 50, batch_size: int = 16\n",
    "):\n",
    "    rewards_history = []\n",
    "    loss_history = []\n",
    "    steps_history = []\n",
    "\n",
    "    best_average_reward = 0\n",
    "\n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        metrics = train_policy_on_batch(env, policy, device, batch_size)\n",
    "\n",
    "        # Collect metrics\n",
    "        rewards_history.append(metrics[\"batch_reward\"])\n",
    "        loss_history.append(metrics[\"policy_loss\"])\n",
    "        steps_history.append(metrics[\"total_steps\"])\n",
    "\n",
    "        # Print metrics every 50 episodes\n",
    "        avg_reward = np.mean(rewards_history[-50:])\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print(\n",
    "                f\"Episode {episode}: Average Reward: {avg_reward:.2f}, \"\n",
    "                f\"Loss: {metrics['policy_loss']:.4f}, Steps: {metrics['total_steps']}\"\n",
    "            )\n",
    "\n",
    "        # Convergence condition: Check if the rolling average exceeds the reward threshold\n",
    "        if len(rewards_history) >= rolling_window:\n",
    "            avg_rolling_reward = np.mean(rewards_history[-rolling_window:])\n",
    "            if avg_rolling_reward >= reward_threshold:\n",
    "                print(\n",
    "                    f\"Environment solved in {episode} episodes! \"\n",
    "                    f\"Average reward over the last {rolling_window} episodes: {avg_rolling_reward:.2f}\"\n",
    "                )\n",
    "                break\n",
    "        if avg_reward >= best_average_reward:\n",
    "            best_average_reward = avg_reward\n",
    "\n",
    "            # Create a filename that includes both the steps_done and timestamp\n",
    "            filename = f\"output/policy_network_lunar_lander_v3_bs_{batch_size}_{timestamp}.pth\"\n",
    "\n",
    "            # Save the policy network with the dynamically generated filename\n",
    "            torch.save(policy.policy_network.state_dict(), filename)\n",
    "\n",
    "            print(f\"Average reward: {avg_reward}. Model saved as: {filename}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return rewards_history, loss_history, steps_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 100\n",
    "reward_threshold = 200.0\n",
    "batch_size = 16\n",
    "\n",
    "rewards_history, loss_history, steps_history = train_vpg_policy(\n",
    "    max_episodes=max_episodes, reward_threshold=reward_threshold, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rewards_history, label=\"Reward per Episode\")\n",
    "plt.axhline(y=reward_threshold, color=\"r\", linestyle=\"--\", label=\"Reward Threshold\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(loss_history, label=\"Loss per Episode\", color=\"orange\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Policy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot steps\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(steps_history, label=\"Steps per Episode\", color=\"green\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.title(\"Steps Taken\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
