{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of possible actions: 4\n",
      "Actions:\n",
      "0: do nothing\n",
      "1: fire left orientation engine\n",
      "2: fire main engine\n",
      "3: fire right orientation engine\n",
      "\n",
      "Number of state observations: 8\n",
      "State (Observation Space):\n",
      "x, y\n",
      "vel_x, vel_y\n",
      "angle, angle_vel\n",
      "left_leg_touching, right_leg_touching\n",
      "      \n",
      "Current state:  [ 0.00225029  1.4121264   0.22791049  0.05360668 -0.00260069 -0.05162515\n",
      "  0.          0.        ]\n",
      "Units of the state are as follows:\n",
      "      ‘x’: (units), ‘y’: (units), \n",
      "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
      "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\", continuous=False, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = int(env.action_space.n)\n",
    "print(f\"Number of possible actions: {n_actions}\")\n",
    "print(\"\"\"Actions:\n",
    "0: do nothing\n",
    "1: fire left orientation engine\n",
    "2: fire main engine\n",
    "3: fire right orientation engine\n",
    "\"\"\")\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(f\"Number of state observations: {n_observations}\")\n",
    "\n",
    "print(\"\"\"State (Observation Space):\n",
    "x, y\n",
    "vel_x, vel_y\n",
    "angle, angle_vel\n",
    "left_leg_touching, right_leg_touching\n",
    "      \"\"\")\n",
    "print(\"Current state: \", state)\n",
    "\n",
    "print(\"\"\"Units of the state are as follows:\n",
    "      ‘x’: (units), ‘y’: (units), \n",
    "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
    "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment to get the initial state\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(50):\n",
    "    env.step(action=0)\n",
    "# Render the environment to get an RGB image\n",
    "frame = env.render()\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Lunar Lander Environment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VPG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam, Optimizer\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "from models import PolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d5bf58212b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGPolicy:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int,\n",
    "        num_outputs: int,\n",
    "        device: torch.device,\n",
    "        learning_rate: float = 0.003,\n",
    "        discount_factor: float = 0.99,\n",
    "    ) -> None:\n",
    "        self.device = device\n",
    "\n",
    "        self.policy_network = PolicyNetwork(num_inputs, num_outputs)\n",
    "        self.policy_network.to(self.device)\n",
    "\n",
    "        self.optimizer = Adam(params=self.policy_network.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.gamma = discount_factor\n",
    "\n",
    "    def select_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state and computes its log-probability.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The current state represented as a tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, torch.Tensor]:\n",
    "                - action (int): The chosen action index.\n",
    "                - log_prob (torch.Tensor): The log-probability of the chosen action.\n",
    "        \"\"\"\n",
    "        # Forward pass through the policy network to get action logits.\n",
    "        action_logits = self.policy_network(state)\n",
    "\n",
    "        # Create a categorical distribution from the logits.\n",
    "        action_distribution = Categorical(logits=action_logits)\n",
    "\n",
    "        # Sample an action from the distribution.\n",
    "        sampled_action = action_distribution.sample()\n",
    "\n",
    "        # Convert the sampled action to a Python integer.\n",
    "        action_index = int(sampled_action.item())\n",
    "\n",
    "        # Calculate the log-probability of the selected action.\n",
    "        log_prob = action_distribution.log_prob(sampled_action)\n",
    "\n",
    "        return action_index, log_prob\n",
    "\n",
    "    def compute_discounted_returns(self, rewards: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes discounted returns for a sequence of rewards.\n",
    "\n",
    "        Args:\n",
    "            rewards (torch.Tensor): Rewards for the episode.\n",
    "            gamma (float): Discount factor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Discounted returns.\n",
    "        \"\"\"\n",
    "        discounted_returns = []\n",
    "        cumulative_return = 0.0\n",
    "        for reward in reversed(rewards):\n",
    "            cumulative_return = reward + self.gamma * cumulative_return\n",
    "            discounted_returns.insert(0, cumulative_return)\n",
    "        discounted_returns = torch.tensor(discounted_returns, dtype=torch.float32)\n",
    "        return (discounted_returns - discounted_returns.mean()) / (discounted_returns.std() + 1e-8)\n",
    "\n",
    "    def calculate_policy_loss(\n",
    "        self, episode_log_probability_actions: torch.Tensor, episode_action_rewards: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"\n",
    "        The loss is negated because most optimization libraries (like PyTorch) perform minimization,\n",
    "        while the policy gradient aims to maximize the objective J(θ).\n",
    "        \"\"\"\n",
    "        return -(episode_log_probability_actions * episode_action_rewards).mean()\n",
    "\n",
    "    def optimize_policy(self, episode_log_probability_actions: torch.Tensor, episode_action_rewards: torch.Tensor):\n",
    "        loss = self.calculate_policy_loss(episode_log_probability_actions, episode_action_rewards)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "discount_factor = 0.99\n",
    "\n",
    "policy = VPGPolicy(\n",
    "    num_inputs=n_observations,\n",
    "    num_outputs=n_actions,\n",
    "    learning_rate=learning_rate,\n",
    "    device=device,\n",
    "    discount_factor=discount_factor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_on_batch(env: gym.Env, policy: VPGPolicy, device: torch.device, batch_size: int = 16) -> dict:\n",
    "    \"\"\"\n",
    "    Collects data from multiple episodes and trains the policy on the combined batch.\n",
    "\n",
    "    Args:\n",
    "        env: The environment to interact with (following the OpenAI Gym interface).\n",
    "        policy: The policy object that defines action selection and optimization.\n",
    "        device: The device to run computations on (CPU/GPU).\n",
    "        batch_size: Number of episodes to collect before updating the policy.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of metrics tracking batch performance and training progress.\n",
    "    \"\"\"\n",
    "    # Lists to store batch data\n",
    "    batch_log_probs = []\n",
    "    batch_rewards = []\n",
    "    batch_returns = []\n",
    "    total_rewards = []  # To track episode rewards for logging\n",
    "    total_steps = 0  # To track steps taken across episodes\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        # Reset the environment for a new episode\n",
    "        state, _ = env.reset()\n",
    "        episode_log_probs = []\n",
    "        episode_rewards = []\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while True:\n",
    "            # Convert state to tensor and send it to the device\n",
    "            state_tensor = torch.Tensor(state).to(device)\n",
    "\n",
    "            # Select an action using the policy\n",
    "            action, log_prob = policy.select_action(state=state_tensor)\n",
    "\n",
    "            # Take the selected action in the environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Store log-probability and reward\n",
    "            episode_log_probs.append(log_prob)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # Update cumulative reward and state\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            total_steps += 1\n",
    "\n",
    "            # Break if the episode ends\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Store episode data into batch data\n",
    "        total_rewards.append(episode_reward)\n",
    "        batch_log_probs.extend(episode_log_probs)\n",
    "        batch_rewards.extend(episode_rewards)\n",
    "\n",
    "        # Compute discounted returns for the episode and store them\n",
    "        episode_returns = policy.compute_discounted_returns(torch.Tensor(episode_rewards))\n",
    "        batch_returns.extend(episode_returns)\n",
    "\n",
    "    # Convert batch data to tensors\n",
    "    batch_log_probs_tensor = torch.stack(batch_log_probs).to(device)\n",
    "    batch_returns_tensor = torch.Tensor(batch_returns).to(device)\n",
    "\n",
    "    # Optimize the policy\n",
    "    policy_loss = policy.optimize_policy(batch_log_probs_tensor, batch_returns_tensor)\n",
    "\n",
    "    # Return metrics to track training progress\n",
    "    metrics = {\n",
    "        \"batch_reward\": sum(total_rewards) / batch_size,  # Average reward per episode\n",
    "        \"total_steps\": total_steps // batch_size,\n",
    "        \"policy_loss\": policy_loss,\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vpg_policy(\n",
    "    max_episodes: int = 500, reward_threshold: float = 200.0, rolling_window: int = 50, batch_size: int = 16\n",
    "):\n",
    "    rewards_history = []\n",
    "    loss_history = []\n",
    "    steps_history = []\n",
    "\n",
    "    best_average_reward = 0\n",
    "\n",
    "    for episode in range(1, max_episodes + 1):\n",
    "        metrics = train_policy_on_batch(env, policy, device, batch_size)\n",
    "\n",
    "        # Collect metrics\n",
    "        rewards_history.append(metrics[\"batch_reward\"])\n",
    "        loss_history.append(metrics[\"policy_loss\"])\n",
    "        steps_history.append(metrics[\"total_steps\"])\n",
    "\n",
    "        # Print metrics every 50 episodes\n",
    "        avg_reward = np.mean(rewards_history[-50:])\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print(\n",
    "                f\"Episode {episode}: Average Reward: {avg_reward:.2f}, \"\n",
    "                f\"Loss: {metrics['policy_loss']:.4f}, Steps: {metrics['total_steps']}\"\n",
    "            )\n",
    "\n",
    "        # Convergence condition: Check if the rolling average exceeds the reward threshold\n",
    "        if len(rewards_history) >= rolling_window:\n",
    "            avg_rolling_reward = np.mean(rewards_history[-rolling_window:])\n",
    "            if avg_rolling_reward >= reward_threshold:\n",
    "                print(\n",
    "                    f\"Environment solved in {episode} episodes! \"\n",
    "                    f\"Average reward over the last {rolling_window} episodes: {avg_rolling_reward:.2f}\"\n",
    "                )\n",
    "                break\n",
    "        if avg_reward >= best_average_reward:\n",
    "            best_average_reward = avg_reward\n",
    "\n",
    "            # Create a filename that includes both the steps_done and timestamp\n",
    "            filename = f\"output/policy_network_lunar_lander_v3_bs_{batch_size}_{timestamp}.pth\"\n",
    "\n",
    "            # Save the policy network with the dynamically generated filename\n",
    "            torch.save(policy.policy_network.state_dict(), filename)\n",
    "\n",
    "            print(f\"Average reward: {avg_reward}. Model saved as: {filename}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return rewards_history, loss_history, steps_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 100\n",
    "reward_threshold = 200.0\n",
    "batch_size = 16\n",
    "\n",
    "rewards_history, loss_history, steps_history = train_vpg_policy(\n",
    "    max_episodes=max_episodes, reward_threshold=reward_threshold, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rewards_history, label=\"Reward per Episode\")\n",
    "plt.axhline(y=reward_threshold, color=\"r\", linestyle=\"--\", label=\"Reward Threshold\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Episode Rewards\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(loss_history, label=\"Loss per Episode\", color=\"orange\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Policy Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot steps\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(steps_history, label=\"Steps per Episode\", color=\"green\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.title(\"Steps Taken\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
