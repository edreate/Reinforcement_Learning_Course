{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state observations: 8\n",
      "State (Observation Space):\n",
      "x, y\n",
      "vel_x, vel_y\n",
      "angle, angle_vel\n",
      "left_leg_touching, right_leg_touching\n",
      "      \n",
      "Current state:  [ 0.00222149  1.4170508   0.22499315  0.27247924 -0.00256733 -0.05096437\n",
      "  0.          0.        ]\n",
      "Units of the state are as follows:\n",
      "      ‘x’: (units), ‘y’: (units), \n",
      "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
      "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "# Select Lunar Lander v3 as environment\n",
    "env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(f\"Number of state observations: {n_observations}\")\n",
    "\n",
    "print(\"\"\"State (Observation Space):\n",
    "x, y\n",
    "vel_x, vel_y\n",
    "angle, angle_vel\n",
    "left_leg_touching, right_leg_touching\n",
    "      \"\"\")\n",
    "print(\"Current state: \", state)\n",
    "\n",
    "print(\"\"\"Units of the state are as follows:\n",
    "      ‘x’: (units), ‘y’: (units), \n",
    "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
    "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reset the environment to get the initial state\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(40):\n",
    "    env.step(action=[0, 0])\n",
    "# Render the environment to get an RGB image\n",
    "frame = env.render()\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Lunar Lander Environment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Type\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models import ValueNetwork\n",
    "from models.VPG import ContinuousPolicyNetwork, DiscretePolicyNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def plot_metrics(episode_durations, rewards, policy_losses, value_losses, show_result=False, save_path=None):\n",
    "    # Create a figure with a 2x2 grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10), dpi=100)\n",
    "    fig.suptitle(\"Training Metrics\" if not show_result else \"Results\", fontsize=16)\n",
    "\n",
    "    # Plot Episode Durations\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    axes[0, 0].set_title(\"Episode Durations\")\n",
    "    axes[0, 0].set_xlabel(\"Episode\")\n",
    "    axes[0, 0].set_ylabel(\"Duration\")\n",
    "    axes[0, 0].plot(durations_t.numpy(), label=\"Duration\")\n",
    "\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        axes[0, 0].plot(means.numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Plot Rewards\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float)\n",
    "    axes[0, 1].set_title(\"Rewards\")\n",
    "    axes[0, 1].set_xlabel(\"Episode\")\n",
    "    axes[0, 1].set_ylabel(\"Reward\")\n",
    "    axes[0, 1].plot(rewards_t.numpy(), label=\"Reward\")\n",
    "\n",
    "    if len(rewards_t) >= 100:\n",
    "        reward_means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        reward_means = torch.cat((torch.zeros(99), reward_means))\n",
    "        axes[0, 1].plot(reward_means.numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # Plot Policy Loss\n",
    "    policy_t = torch.tensor(policy_losses, dtype=torch.float)\n",
    "    axes[1, 0].set_title(\"Policy Loss\")\n",
    "    axes[1, 0].set_xlabel(\"Episode\")\n",
    "    axes[1, 0].set_ylabel(\"Loss\")\n",
    "    axes[1, 0].plot(policy_t.numpy(), label=\"Policy Loss\", color=\"orange\")\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # Plot Value Loss\n",
    "    value_t = torch.tensor(value_losses, dtype=torch.float)\n",
    "    axes[1, 1].set_title(\"Value Loss\")\n",
    "    axes[1, 1].set_xlabel(\"Episode\")\n",
    "    axes[1, 1].set_ylabel(\"Loss\")\n",
    "    axes[1, 1].plot(value_t.numpy(), label=\"Value Loss\", color=\"green\")\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "    # Adjust layout and save/show\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add space for the suptitle\n",
    "    if save_path:\n",
    "        plt.savefig(save_path + \".png\", dpi=300)\n",
    "        print(f\"Metrics figure saved to {save_path}\")\n",
    "\n",
    "    if \"get_ipython\" in globals():\n",
    "        if not show_result:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(fig)\n",
    "        else:\n",
    "            display.display(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "N_ACTIONS = 2\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Value Network\n",
    "\n",
    "We’ll create a simple feedforward neural network for the state-value function. This acts as our baseline for reducing variance in the policy gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 256, 256]           2,304\n",
      "            Linear-2             [-1, 256, 256]          65,792\n",
      "            Linear-3               [-1, 256, 1]             257\n",
      "================================================================\n",
      "Total params: 68,353\n",
      "Trainable params: 68,353\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 0.26\n",
      "Estimated Total Size (MB): 1.27\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edreate/Desktop/Reinforcement_Learning_Course/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "value_network = ValueNetwork(n_observations, 256)\n",
    "\n",
    "# Generate a summary of the model\n",
    "summary(value_network, input_size=(BATCH_SIZE, n_observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discrete Policy Network\n",
    "\n",
    "For discrete action spaces, the policy outputs a probability distribution over the discrete actions (e.g., using a Softmax). We’ll return a Categorical distribution from which we can sample actions.\n",
    "\n",
    "For continuous action spaces, the policy outputs the mean and log_std of a Gaussian distribution. We can then sample from this Gaussian distribution to get actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 256, 256]           2,304\n",
      "            Linear-2             [-1, 256, 256]          65,792\n",
      "            Linear-3               [-1, 256, 1]             257\n",
      "================================================================\n",
      "Total params: 68,353\n",
      "Trainable params: 68,353\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.00\n",
      "Params size (MB): 0.26\n",
      "Estimated Total Size (MB): 1.27\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "policy_network = ContinuousPolicyNetwork(n_observations, N_ACTIONS, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "summary(value_network, input_size=(BATCH_SIZE, n_observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collect Trajectory\n",
    "\n",
    "We will write a helper function to collect trajectories. This function will:\n",
    "\n",
    "- Reset the environment.\n",
    "- Step through the environment using the current policy.\n",
    "- Store (state, action, reward, log_prob, done) until the episode ends.\n",
    "- Return lists of states, actions, log_probs, rewards, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(env: gym.Env, policy_net: Type[nn.Module], value_net: Type[nn.Module], max_steps: int = 1000):\n",
    "    \"\"\"\n",
    "    Collects one trajectory (episode) given a policy.\n",
    "    Returns lists of states, actions, log_probs, rewards, done_flags, and values.\n",
    "    \"\"\"\n",
    "    global steps_taken\n",
    "    states = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "    values = []\n",
    "\n",
    "    state = env.reset()\n",
    "    for _ in range(max_steps):\n",
    "        # Get action and log_prob from policy\n",
    "        action, log_prob = policy_net.get_action(state)\n",
    "\n",
    "        # Value of the current state\n",
    "        state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "        value = value_net(state_t).item()\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        steps_taken += 1\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Record\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        values.append(value)\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, log_probs, rewards, dones, values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advantage Calculation\n",
    "\n",
    "We’ll compute the return (cumulative discounted reward) from each timestep, then compute the advantages by subtracting the value function (baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantages(rewards: List[float], values: List[float], dones: List[bool], gamma: float = 0.99):\n",
    "    \"\"\"\n",
    "    Compute advantage using: Gt - V(st),\n",
    "    where Gt is the discounted return from time t.\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays for convenience\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    values = np.array(values, dtype=np.float32)\n",
    "    dones = np.array(dones, dtype=np.bool)\n",
    "\n",
    "    # Bootstrapped final value is zero for an episode\n",
    "    # (or you could do a value_net(next_state) if continuing)\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r, done in zip(rewards[::-1], dones[::-1]):\n",
    "        if done:\n",
    "            G = 0\n",
    "        G = r + gamma * G\n",
    "        returns.append(G)\n",
    "    returns.reverse()\n",
    "    returns = np.array(returns, dtype=np.float32)\n",
    "\n",
    "    advantages = returns - values\n",
    "    return returns, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vpg_continuous(\n",
    "    env: gym.Env, state_dim, action_dim, hidden_dim=256, lr_policy=1e-3, lr_value=1e-3, gamma=0.99, max_episodes=1000\n",
    "):\n",
    "    global episode_durations, episode_rewards, policy_losses, value_losses\n",
    "\n",
    "    # Initialize networks\n",
    "    policy_net = ContinuousPolicyNetwork(state_dim, action_dim, hidden_dim)\n",
    "    value_net = ValueNetwork(state_dim, hidden_dim)\n",
    "\n",
    "    # Optimizers\n",
    "    policy_optimizer = optim.Adam(policy_net.parameters(), lr=lr_policy)\n",
    "    value_optimizer = optim.Adam(value_net.parameters(), lr=lr_value)\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        # Collect one trajectory\n",
    "        (states, actions, log_probs, rewards, dones, values) = collect_trajectory(env, policy_net, value_net)\n",
    "\n",
    "        # Compute returns and advantages\n",
    "        returns, advantages = compute_advantages(rewards, values, dones, gamma)\n",
    "\n",
    "        # Convert everything to torch tensors\n",
    "        log_probs_tensor = torch.stack(log_probs)\n",
    "        advantages_tensor = torch.FloatTensor(advantages)\n",
    "        returns_tensor = torch.FloatTensor(returns)\n",
    "\n",
    "        # 1. Update Policy (maximize log_prob * advantage)\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss = -(log_probs_tensor * advantages_tensor).mean()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        # 2. Update Value Function (fit to returns)\n",
    "        value_optimizer.zero_grad()\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        value_preds = value_net(states_tensor).squeeze()\n",
    "        value_loss = nn.MSELoss()(value_preds, returns_tensor)\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        ep_return = np.sum(rewards)\n",
    "        print(f\"Episode {episode}, Return: {ep_return:.2f}\")\n",
    "\n",
    "        episode_durations.append(len(dones))\n",
    "        episode_rewards.append(sum(rewards))\n",
    "        policy_losses.append(policy_loss.item())\n",
    "        value_losses.append(value_loss.item())\n",
    "\n",
    "        plot_metrics(episode_durations, episode_rewards, policy_losses, value_losses)\n",
    "\n",
    "    env.close()\n",
    "    return policy_net, value_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edreate/Desktop/Reinforcement_Learning_Course/models/VPG.py:67: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  state = torch.FloatTensor(state).unsqueeze(0)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 8 at dim 1 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m steps_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msac_checkpoint_best\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_vpg_continuous\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_observations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_ACTIONS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m, in \u001b[0;36mtrain_vpg_continuous\u001b[0;34m(env, state_dim, action_dim, hidden_dim, lr_policy, lr_value, gamma, max_episodes)\u001b[0m\n\u001b[1;32m     18\u001b[0m value_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(value_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr_value)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_episodes):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Collect one trajectory\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     (states, actions, log_probs, rewards, dones, values) \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_net\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Compute returns and advantages\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     returns, advantages \u001b[38;5;241m=\u001b[39m compute_advantages(rewards, values, dones, gamma)\n",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m, in \u001b[0;36mcollect_trajectory\u001b[0;34m(env, policy_net, value_net, max_steps)\u001b[0m\n\u001b[1;32m     14\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Get action and log_prob from policy\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Value of the current state\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     state_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Reinforcement_Learning_Course/models/VPG.py:67\u001b[0m, in \u001b[0;36mContinuousPolicyNetwork.get_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    state: single state, shape [state_dim]\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    returns: sampled_action (numpy array), log_prob_of_that_action\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     68\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(state)\n\u001b[1;32m     69\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 8 at dim 1 (got 0)"
     ]
    }
   ],
   "source": [
    "episode_durations = []\n",
    "episode_rewards = []\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "steps_taken = 0\n",
    "model_name = \"sac_checkpoint_best\"\n",
    "\n",
    "train_vpg_continuous(env, n_observations, N_ACTIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
