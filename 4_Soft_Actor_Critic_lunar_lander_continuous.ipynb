{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic\n",
    "\n",
    "#### Off Policy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Continuous Actions:**  Use a `Box(-1, +1, (2,), dtype=np.float32): (np.array([main, lateral]))` action space with two components: \n",
    "\n",
    "1. **Main Engine Throttle (`main`):**  \n",
    "   1. - **Off:**  If `main < 0`.\n",
    "   2. - **Throttle Scaling:**  Increases linearly from 50% to 100% as `main` ranges from 0 to 1.\n",
    "   - **Note:**  The main engine does not operate below 50% power.\n",
    " \n",
    "2. **Lateral Boosters Throttle (`lateral`):**\n",
    "   1. - **Inactive:**  If `-0.5 < lateral < 0.5`.\n",
    "   2. - **Left Booster:**  Activates when `lateral < -0.5`, scaling throttle from 50% to 100% as `lateral` decreases from -0.5 to -1.\n",
    "   3. - **Right Booster:**  Activates when `lateral > 0.5`, scaling throttle from 50% to 100% as `lateral` increases from 0.5 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Lunar Lander v3 as environment\n",
    "env = gym.make(\"LunarLander-v3\", continuous=True, render_mode=\"rgb_array\")\n",
    "\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "print(f\"Number of state observations: {n_observations}\")\n",
    "\n",
    "print(\"\"\"State (Observation Space):\n",
    "x, y\n",
    "vel_x, vel_y\n",
    "angle, angle_vel\n",
    "left_leg_touching, right_leg_touching\n",
    "      \"\"\")\n",
    "print(\"Current state: \", state)\n",
    "\n",
    "print(\"\"\"Units of the state are as follows:\n",
    "      ‘x’: (units), ‘y’: (units), \n",
    "      ‘vx’: (units/second), ‘vy’: (units/second), \n",
    "      ‘angle’: (radians), ‘angular velocity’: (radians/second)\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment to get the initial state\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(40):\n",
    "    env.step(action=[0, 0])\n",
    "# Render the environment to get an RGB image\n",
    "frame = env.render()\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Lunar Lander Environment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def plot_metrics(episode_durations, rewards, policy_losses, value_losses, show_result=False, save_path=None):\n",
    "    # Create a figure with a 2x2 grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10), dpi=100)\n",
    "    fig.suptitle(\"Training Metrics\" if not show_result else \"Results\", fontsize=16)\n",
    "\n",
    "    # Plot Episode Durations\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    axes[0, 0].set_title(\"Episode Durations\")\n",
    "    axes[0, 0].set_xlabel(\"Episode\")\n",
    "    axes[0, 0].set_ylabel(\"Duration\")\n",
    "    axes[0, 0].plot(durations_t.cpu().numpy(), label=\"Duration\")\n",
    "\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        axes[0, 0].plot(means.cpu().numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "    # Plot Rewards\n",
    "    rewards_t = torch.tensor(rewards, dtype=torch.float)\n",
    "    axes[0, 1].set_title(\"Rewards\")\n",
    "    axes[0, 1].set_xlabel(\"Episode\")\n",
    "    axes[0, 1].set_ylabel(\"Reward\")\n",
    "    axes[0, 1].plot(rewards_t.cpu().numpy(), label=\"Reward\")\n",
    "\n",
    "    if len(rewards_t) >= 100:\n",
    "        reward_means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        reward_means = torch.cat((torch.zeros(99), reward_means))\n",
    "        axes[0, 1].plot(reward_means.cpu().numpy(), label=\"100-Episode Avg\", linestyle=\"--\")\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # Plot Policy Loss\n",
    "    policy_t = torch.tensor(policy_losses, dtype=torch.float)\n",
    "    axes[1, 0].set_title(\"Policy Loss\")\n",
    "    axes[1, 0].set_xlabel(\"Episode\")\n",
    "    axes[1, 0].set_ylabel(\"Loss\")\n",
    "    axes[1, 0].plot(policy_t.cpu().numpy(), label=\"Policy Loss\", color=\"orange\")\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "    # Plot Value Loss\n",
    "    value_t = torch.tensor(value_losses, dtype=torch.float)\n",
    "    axes[1, 1].set_title(\"Value Loss\")\n",
    "    axes[1, 1].set_xlabel(\"Episode\")\n",
    "    axes[1, 1].set_ylabel(\"Loss\")\n",
    "    axes[1, 1].plot(value_t.cpu().numpy(), label=\"Value Loss\", color=\"green\")\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "    # Adjust layout and save/show\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Add space for the suptitle\n",
    "    if save_path:\n",
    "        plt.savefig(save_path + \".png\", dpi=300)\n",
    "        print(f\"Metrics figure saved to {save_path}\")\n",
    "\n",
    "    if \"get_ipython\" in globals():\n",
    "        if not show_result:\n",
    "            display.clear_output(wait=True)\n",
    "            display.display(fig)\n",
    "        else:\n",
    "            display.display(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Tuple, Type\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import json\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ReplayBuffer import ReplayBuffer\n",
    "from models.SAC import ActorNetwork\n",
    "from models.models import CriticNetwork, ValueNetwork\n",
    "from models.utils import update_target_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACLosses:\n",
    "    def __init__(self, alpha: float = 0.2, automatic_entropy: bool = True, target_entropy: float = -1.0):\n",
    "        \"\"\"\n",
    "        Initializes the loss computation and entropy coefficient handling.\n",
    "\n",
    "        Args:\n",
    "            alpha (float): Initial entropy coefficient.\n",
    "            automatic_entropy (bool): Whether to use automatic entropy adjustment.\n",
    "            target_entropy (float): Target entropy value.\n",
    "        \"\"\"\n",
    "        self.automatic_entropy = automatic_entropy\n",
    "        if automatic_entropy:\n",
    "            self.log_alpha = torch.tensor(\n",
    "                [torch.log(torch.tensor(alpha))],\n",
    "                requires_grad=True,\n",
    "                device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            )\n",
    "            self.target_entropy = target_entropy\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "\n",
    "    def get_alpha(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the current entropy coefficient.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Entropy coefficient.\n",
    "        \"\"\"\n",
    "        return self.log_alpha.exp() if self.automatic_entropy else torch.tensor(self.alpha)\n",
    "\n",
    "    def alpha_loss(self, log_prob: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the loss for the entropy coefficient.\n",
    "\n",
    "        Args:\n",
    "            log_prob (torch.Tensor): Log probabilities of actions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Loss for adjusting the entropy coefficient.\n",
    "        \"\"\"\n",
    "        if self.automatic_entropy:\n",
    "            return -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n",
    "        return torch.tensor(0.0, device=log_prob.device)\n",
    "\n",
    "\n",
    "def compute_actor_loss(\n",
    "    actor: Type[nn.Module],\n",
    "    critic_1: Type[nn.Module],\n",
    "    critic_2: Type[nn.Module],\n",
    "    states: torch.Tensor,\n",
    "    alpha: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the actor loss using the reparameterization trick.\n",
    "\n",
    "    Args:\n",
    "        actor (ActorNetwork): The actor network.\n",
    "        critic_1 (CriticNetwork): The first critic network.\n",
    "        critic_2 (CriticNetwork): The second critic network.\n",
    "        states (torch.Tensor): Batch of states.\n",
    "        alpha (torch.Tensor): Entropy coefficient.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Actor loss.\n",
    "    \"\"\"\n",
    "    actions, log_probs = actor.sample_action(states)\n",
    "    q1 = critic_1(states, actions)\n",
    "    q2 = critic_2(states, actions)\n",
    "    q_min = torch.min(q1, q2)\n",
    "    return (alpha * log_probs - q_min).mean()\n",
    "\n",
    "\n",
    "def compute_critic_loss(\n",
    "    critic: Type[nn.Module],\n",
    "    target_value_net: Type[nn.Module],\n",
    "    states: torch.Tensor,\n",
    "    actions: torch.Tensor,\n",
    "    rewards: torch.Tensor,\n",
    "    next_states: torch.Tensor,\n",
    "    dones: torch.Tensor,\n",
    "    gamma: float,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the critic loss using the TD error.\n",
    "\n",
    "    Args:\n",
    "        critic (CriticNetwork): The critic network.\n",
    "        target_value_net (ValueNetwork): The target value network.\n",
    "        states (torch.Tensor): Batch of states.\n",
    "        actions (torch.Tensor): Batch of actions.\n",
    "        rewards (torch.Tensor): Batch of rewards.\n",
    "        next_states (torch.Tensor): Batch of next states.\n",
    "        dones (torch.Tensor): Batch of done flags.\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Critic loss.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        target_value = target_value_net(next_states)\n",
    "        target_q = rewards + gamma * (1 - dones) * target_value\n",
    "    predicted_q = critic(states, actions)\n",
    "    return nn.MSELoss()(predicted_q, target_q)\n",
    "\n",
    "\n",
    "def compute_value_loss(\n",
    "    value_net: Type[nn.Module],\n",
    "    critic_1: Type[nn.Module],\n",
    "    critic_2: Type[nn.Module],\n",
    "    states: torch.Tensor,\n",
    "    alpha: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the value loss using the Bellman equation.\n",
    "\n",
    "    Args:\n",
    "        value_net (ValueNetwork): The value network.\n",
    "        critic_1 (CriticNetwork): The first critic network.\n",
    "        critic_2 (CriticNetwork): The second critic network.\n",
    "        states (torch.Tensor): Batch of states.\n",
    "        alpha (torch.Tensor): Entropy coefficient.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Value loss.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        actions, log_probs = actor.sample_action(states)\n",
    "        q1 = critic_1(states, actions)\n",
    "        q2 = critic_2(states, actions)\n",
    "        q_min = torch.min(q1, q2)\n",
    "        target_value = q_min - alpha * log_probs\n",
    "    predicted_value = value_net(states)\n",
    "    return nn.MSELoss()(predicted_value, target_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and Target Network Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_optimizers(\n",
    "    actor: Type[nn.Module],\n",
    "    critic_1: Type[nn.Module],\n",
    "    critic_2: Type[nn.Module],\n",
    "    value_net: Type[nn.Module],\n",
    "    learning_rate: float = 3e-4,\n",
    ") -> Tuple[optim.Optimizer, optim.Optimizer, optim.Optimizer, optim.Optimizer]:\n",
    "    \"\"\"\n",
    "    Sets up optimizers for the actor, critic, and value networks.\n",
    "\n",
    "    Args:\n",
    "        actor (ActorNetwork): The actor network.\n",
    "        critic_1 (CriticNetwork): The first critic network.\n",
    "        critic_2 (CriticNetwork): The second critic network.\n",
    "        value_net (ValueNetwork): The value network.\n",
    "        learning_rate (float): Learning rate for the optimizers.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[optim.Optimizer, optim.Optimizer, optim.Optimizer, optim.Optimizer]: Optimizers for actor, critics, and value networks.\n",
    "    \"\"\"\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "    critic_1_optimizer = optim.Adam(critic_1.parameters(), lr=learning_rate)\n",
    "    critic_2_optimizer = optim.Adam(critic_2.parameters(), lr=learning_rate)\n",
    "    value_optimizer = optim.Adam(value_net.parameters(), lr=learning_rate)\n",
    "    return actor_optimizer, critic_1_optimizer, critic_2_optimizer, value_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(\n",
    "    directory: str,\n",
    "    actor: Type[nn.Module],\n",
    "    critic_1: Type[nn.Module],\n",
    "    critic_2: Type[nn.Module],\n",
    "    value_net: Type[nn.Module],\n",
    "    target_value_net: Type[nn.Module],\n",
    "    actor_optimizer: torch.optim.Optimizer,\n",
    "    critic_1_optimizer: torch.optim.Optimizer,\n",
    "    critic_2_optimizer: torch.optim.Optimizer,\n",
    "    value_optimizer: torch.optim.Optimizer,\n",
    "    episode: int,\n",
    "    steps_taken: int,\n",
    "    latest_episode_reward: float,\n",
    "    model_name: Optional[str] = \"sac_checkpoint_best\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Saves SAC models, optimizers, and additional metadata.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Directory to save models and metadata.\n",
    "        actor, critic_1, critic_2, value_net, target_value_net: Neural networks.\n",
    "        optimizers: PyTorch optimizers for each network.\n",
    "        episode (int): Current training episode.\n",
    "        steps_taken (int): Total steps taken so far.\n",
    "        latest_episode_reward (float): Reward from the latest episode.\n",
    "        model_name (str): Name of the checkpoint file (without extension).\n",
    "    \"\"\"\n",
    "    # Use pathlib to create the directory\n",
    "    path = Path(directory)\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    checkpoint_path = path / f\"{model_name}.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"actor_state_dict\": actor.state_dict(),\n",
    "            \"critic_1_state_dict\": critic_1.state_dict(),\n",
    "            \"critic_2_state_dict\": critic_2.state_dict(),\n",
    "            \"value_net_state_dict\": value_net.state_dict(),\n",
    "            \"target_value_net_state_dict\": target_value_net.state_dict(),\n",
    "            \"actor_optimizer_state_dict\": actor_optimizer.state_dict(),\n",
    "            \"critic_1_optimizer_state_dict\": critic_1_optimizer.state_dict(),\n",
    "            \"critic_2_optimizer_state_dict\": critic_2_optimizer.state_dict(),\n",
    "            \"value_optimizer_state_dict\": value_optimizer.state_dict(),\n",
    "            \"episode\": episode,\n",
    "        },\n",
    "        checkpoint_path,\n",
    "    )\n",
    "    print(f\"Models saved to {checkpoint_path}\")\n",
    "\n",
    "    # Save metadata as JSON\n",
    "    metadata = {\n",
    "        \"episode\": episode,\n",
    "        \"steps_taken\": steps_taken,\n",
    "        \"latest_episode_reward\": latest_episode_reward,\n",
    "    }\n",
    "    metadata_path = path / f\"{model_name}_metadata.json\"\n",
    "    with metadata_path.open(\"w\") as metadata_file:\n",
    "        json.dump(metadata, metadata_file, indent=4)\n",
    "    print(f\"Metadata saved to {metadata_path}\")\n",
    "\n",
    "\n",
    "def load_models(\n",
    "    checkpoint_path: str,\n",
    "    actor: Type[nn.Module],\n",
    "    critic_1: Type[nn.Module],\n",
    "    critic_2: Type[nn.Module],\n",
    "    value_net: Type[nn.Module],\n",
    "    target_value_net: Type[nn.Module],\n",
    "    actor_optimizer: optim.Optimizer,\n",
    "    critic_1_optimizer: optim.Optimizer,\n",
    "    critic_2_optimizer: optim.Optimizer,\n",
    "    value_optimizer: optim.Optimizer,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Loads SAC models and optimizers from a checkpoint.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "        actor, critic_1, critic_2, value_net, target_value_net: Neural networks.\n",
    "        optimizers: PyTorch optimizers for each network.\n",
    "\n",
    "    Returns:\n",
    "        int: The episode number from the checkpoint.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"No checkpoint found at {checkpoint_path}\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    actor.load_state_dict(checkpoint[\"actor_state_dict\"])\n",
    "    critic_1.load_state_dict(checkpoint[\"critic_1_state_dict\"])\n",
    "    critic_2.load_state_dict(checkpoint[\"critic_2_state_dict\"])\n",
    "    value_net.load_state_dict(checkpoint[\"value_net_state_dict\"])\n",
    "    target_value_net.load_state_dict(checkpoint[\"target_value_net_state_dict\"])\n",
    "\n",
    "    actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer_state_dict\"])\n",
    "    critic_1_optimizer.load_state_dict(checkpoint[\"critic_1_optimizer_state_dict\"])\n",
    "    critic_2_optimizer.load_state_dict(checkpoint[\"critic_2_optimizer_state_dict\"])\n",
    "    value_optimizer.load_state_dict(checkpoint[\"value_optimizer_state_dict\"])\n",
    "\n",
    "    episode = checkpoint[\"episode\"]\n",
    "    print(f\"Models loaded from {checkpoint_path}, resuming at episode {episode}\")\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sac(\n",
    "    env: gym.Env,\n",
    "    actor: Type[nn.Module],\n",
    "    critic_1: Type[nn.Module],\n",
    "    critic_2: Type[nn.Module],\n",
    "    value_net: Type[nn.Module],\n",
    "    target_value_net: Type[nn.Module],\n",
    "    replay_buffer: ReplayBuffer,\n",
    "    sac_losses: SACLosses,\n",
    "    actor_optimizer: optim.Optimizer,\n",
    "    critic_1_optimizer: optim.Optimizer,\n",
    "    critic_2_optimizer: optim.Optimizer,\n",
    "    value_optimizer: optim.Optimizer,\n",
    "    output_dir: str,\n",
    "    model_name: str,\n",
    "    num_episodes: int = 1000,\n",
    "    batch_size: int = 64,\n",
    "    gamma: float = 0.99,\n",
    "    tau: float = 0.005,\n",
    "    evaluate_interval: int = 10,\n",
    "    reward_window_length: int = 50,\n",
    "    reward_window_threshold: float = 200.0,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Trains the SAC agent.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        actor, critic_1, critic_2, value_net, target_value_net: Neural networks.\n",
    "        replay_buffer (ReplayBuffer): Experience replay buffer.\n",
    "        sac_losses (SACLosses): Loss computation module.\n",
    "        optimizers: PyTorch optimizers for each network.\n",
    "        num_episodes (int): Number of training episodes.\n",
    "        batch_size (int): Batch size for training.\n",
    "        gamma (float): Discount factor.\n",
    "        tau (float): Target network update rate.\n",
    "        evaluate_interval (int): Interval for evaluation and visualization.\n",
    "    \"\"\"\n",
    "    global episode_durations, episode_rewards, policy_losses, value_losses, steps_taken\n",
    "\n",
    "    # Select device (GPU or CPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Move models to the selected device\n",
    "    actor.to(device)\n",
    "    critic_1.to(device)\n",
    "    critic_2.to(device)\n",
    "    value_net.to(device)\n",
    "    target_value_net.to(device)\n",
    "\n",
    "    # Create output directory with timestamp\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    best_reward = float(\"-inf\")\n",
    "    average_window_reward = 0.0\n",
    "\n",
    "    progress_bar = tqdm(total=num_episodes, desc=\"Training Progress\", position=0, leave=True)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        total_reward = 0\n",
    "        duration = 0\n",
    "\n",
    "        episode_policy_loss = 0\n",
    "        episode_value_loss = 0\n",
    "\n",
    "        while True:\n",
    "            # Select action using the policy\n",
    "            with torch.no_grad():\n",
    "                action, _ = actor.sample_action(state.unsqueeze(0))\n",
    "                action = action.squeeze(0).cpu().numpy()\n",
    "\n",
    "            # Interact with the environment\n",
    "            next_state, reward, truncated, terminated, _ = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "            total_reward += reward\n",
    "            duration += 1\n",
    "            steps_taken += 1\n",
    "\n",
    "            if truncated or terminated:\n",
    "                done = True\n",
    "            else:\n",
    "                done = False\n",
    "\n",
    "            if duration >= 999:\n",
    "                done = True\n",
    "\n",
    "            # Add experience to the replay buffer\n",
    "            replay_buffer.add(state.cpu().numpy(), action, reward, next_state.cpu().numpy(), done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # Update networks if the buffer is large enough\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                # Sample minibatch\n",
    "                states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "                states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "                actions = torch.tensor(actions, dtype=torch.float32, device=device)\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "                next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "                dones = torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "\n",
    "                # Compute losses\n",
    "                alpha = sac_losses.get_alpha()\n",
    "                actor_loss = compute_actor_loss(actor, critic_1, critic_2, states, alpha)\n",
    "                critic_1_loss = compute_critic_loss(\n",
    "                    critic_1, target_value_net, states, actions, rewards, next_states, dones, gamma\n",
    "                )\n",
    "                critic_2_loss = compute_critic_loss(\n",
    "                    critic_2, target_value_net, states, actions, rewards, next_states, dones, gamma\n",
    "                )\n",
    "                value_loss = compute_value_loss(value_net, critic_1, critic_2, states, alpha)\n",
    "                entropy_loss = sac_losses.alpha_loss(actor.sample_action(states)[1])\n",
    "\n",
    "                # Optimize actor\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "\n",
    "                # Optimize critics\n",
    "                critic_1_optimizer.zero_grad()\n",
    "                critic_1_loss.backward()\n",
    "                critic_1_optimizer.step()\n",
    "\n",
    "                critic_2_optimizer.zero_grad()\n",
    "                critic_2_loss.backward()\n",
    "                critic_2_optimizer.step()\n",
    "\n",
    "                # Optimize value\n",
    "                value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                value_optimizer.step()\n",
    "\n",
    "                # Optimize alpha if using automatic entropy\n",
    "                if sac_losses.automatic_entropy:\n",
    "                    entropy_loss.backward()\n",
    "                    sac_losses.log_alpha.grad -= entropy_loss.detach()\n",
    "\n",
    "                # Update target network\n",
    "                update_target_network(target_value_net, value_net, tau)\n",
    "\n",
    "                episode_policy_loss += actor_loss.item()\n",
    "                episode_value_loss += value_loss.item()\n",
    "\n",
    "                # Update progress bar with current metrics\n",
    "                progress_bar.set_postfix(\n",
    "                    {\n",
    "                        \"Episode\": episode + 1,\n",
    "                        \"Steps Taken\": steps_taken,\n",
    "                        \"Total Reward\": total_reward,\n",
    "                        \"Episode Duration\": duration,\n",
    "                        \"Avg Policy Loss\": np.mean(policy_losses[-10:]) if len(policy_losses) > 10 else 0,\n",
    "                        \"Avg Value Loss\": np.mean(value_losses[-10:]) if len(value_losses) > 10 else 0,\n",
    "                    }\n",
    "                )\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Track episode metrics\n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_durations.append(duration)\n",
    "\n",
    "        # Track losses\n",
    "        policy_losses.append(episode_policy_loss / duration)\n",
    "        value_losses.append(episode_value_loss / duration)\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # if episode > reward_window_length:\n",
    "        #     average_window_reward = sum(episode_rewards[-reward_window_length:]) / reward_window_length\n",
    "\n",
    "        #     if average_window_reward > reward_window_threshold:\n",
    "        #         print(\"Converged. Ending training\")\n",
    "        #         break\n",
    "\n",
    "        # Save the best model\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            save_models(\n",
    "                output_dir,\n",
    "                actor,\n",
    "                critic_1,\n",
    "                critic_2,\n",
    "                value_net,\n",
    "                target_value_net,\n",
    "                actor_optimizer,\n",
    "                critic_1_optimizer,\n",
    "                critic_2_optimizer,\n",
    "                value_optimizer,\n",
    "                episode,\n",
    "                steps_taken,\n",
    "                total_reward,\n",
    "                model_name,\n",
    "            )\n",
    "            print(f\"New best model saved with reward: {best_reward:.2f}\")\n",
    "\n",
    "        plot_metrics(episode_durations, episode_rewards, policy_losses, value_losses)\n",
    "\n",
    "        # # Evaluate periodically\n",
    "        # if episode % evaluate_interval == 0:\n",
    "        #     print(\"Evaluating Policy\")\n",
    "        #     evaluate_sac(env, actor)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sac(env: gym.Env, actor: Type[nn.Module], num_episodes: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the SAC agent.\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): The environment.\n",
    "        actor (ActorNetwork): Trained actor network.\n",
    "        num_episodes (int): Number of evaluation episodes.\n",
    "    \"\"\"\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        total_reward = 0\n",
    "\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                action, _ = actor.sample_action(state.unsqueeze(0))\n",
    "                action = action.squeeze(0).cpu().numpy()\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = torch.tensor(next_state, dtype=torch.float32)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(f\"Evaluation Episode {episode + 1}: Total Reward = {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = n_observations\n",
    "action_dim = 2\n",
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(100000, n_observations, action_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAC Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sac_losses = SACLosses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "actor = ActorNetwork(state_dim, action_dim, hidden_dim)\n",
    "critic_1 = CriticNetwork(state_dim, action_dim, hidden_dim)\n",
    "critic_2 = CriticNetwork(state_dim, action_dim, hidden_dim)\n",
    "value_net = ValueNetwork(state_dim, hidden_dim)\n",
    "\n",
    "# Create target network\n",
    "target_value_net = ValueNetwork(state_dim, hidden_dim)\n",
    "target_value_net.load_state_dict(value_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optimizer, critic_1_optimizer, critic_2_optimizer, value_optimizer = setup_optimizers(\n",
    "    actor, critic_1, critic_2, value_net, learning_rate=3e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES_MAX = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global metrics to track performance\n",
    "episode_durations = []\n",
    "episode_rewards = []\n",
    "policy_losses = []\n",
    "value_losses = []\n",
    "steps_taken = 0\n",
    "model_name = \"sac_checkpoint_best\"\n",
    "\n",
    "# Timestamp for file saving\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "output_dir = f\"pre_trained_models/sac_continuous_lunar_lander_training_{timestamp}\"\n",
    "\n",
    "train_sac(\n",
    "    env,\n",
    "    actor,\n",
    "    critic_1,\n",
    "    critic_2,\n",
    "    value_net,\n",
    "    target_value_net,\n",
    "    replay_buffer,\n",
    "    sac_losses,\n",
    "    actor_optimizer,\n",
    "    critic_1_optimizer,\n",
    "    critic_2_optimizer,\n",
    "    value_optimizer,\n",
    "    output_dir=output_dir,\n",
    "    num_episodes=N_EPISODES_MAX,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    evaluate_interval=10,\n",
    "    reward_window_threshold=230.0,\n",
    "    reward_window_length=100,\n",
    "    model_name=model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(\n",
    "    episode_durations,\n",
    "    episode_rewards,\n",
    "    policy_losses,\n",
    "    value_losses,\n",
    "    show_result=True,\n",
    "    save_path=f\"{output_dir}/soft_actor_critic_continuous_plot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Checkpoint and Save Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_policy_from_checkpoint(checkpoint_path: str, actor: Type[nn.Module], output_path: str = None) -> None:\n",
    "    \"\"\"\n",
    "    Loads a checkpoint and saves only the actor (policy network) as a .pth file.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (str): Path to the checkpoint file.\n",
    "        actor (ActorNetwork): Actor network instance to load the weights into.\n",
    "        output_path (str, optional): Path to save the policy network as a .pth file.\n",
    "                                     Defaults to the same directory with a new filename.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"No checkpoint found at {checkpoint_path}\")\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "\n",
    "    # Load actor state dict\n",
    "    actor.load_state_dict(checkpoint[\"actor_state_dict\"])\n",
    "\n",
    "    # Derive default output path if not provided\n",
    "    if output_path is None:\n",
    "        directory = os.path.dirname(checkpoint_path)\n",
    "        output_path = os.path.join(directory, \"policy_network.pth\")\n",
    "\n",
    "    # Save only the actor's weights\n",
    "    torch.save(actor.state_dict(), output_path)\n",
    "    print(f\"Policy network saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = n_observations\n",
    "action_dim = 2\n",
    "hidden_dim = 256\n",
    "\n",
    "actor_network = ActorNetwork(state_dim, action_dim, hidden_dim)\n",
    "checkpoint_path = \"output/sac_continuous_lunar_lander_training_2024-12-23_12-52-16/sac_checkpoint_best.pt\"\n",
    "\n",
    "output_dir = Path(output_dir)\n",
    "\n",
    "# Save the model checkpoint\n",
    "checkpoint_path = output_dir / f\"{model_name}.pt\"\n",
    "\n",
    "save_policy_from_checkpoint(checkpoint_path, actor=actor_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
